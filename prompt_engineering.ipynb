{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genai import Client, Credentials\n",
    "\n",
    "client = Client(credentials=Credentials.from_env())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GENAI_KEY\"] = \"pak-8prwoIymlhiNFNhxydsvEL8pl2VnQi_84sy03fvyLC0\"\n",
    "os.environ[\"GENAI_API\"] = \"https://bam-api.res.ibm.com/\"\n",
    "\n",
    "from genai.credentials import Credentials\n",
    "\n",
    "credentials = Credentials.from_env()\n",
    "client = Client(credentials=Credentials.from_env())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================== Start conversation with langchain =======================\n",
      "\n",
      "Request: Hi, My name is Saima\n",
      "New conversation with ID '9debc536-f412-462e-9698-d59e7b63e294' has been created!\n",
      "Response:   Hello Saima! It's nice to meet you. Is there anything I can assist you with today? Please feel free to ask me any questions, and I'll do my best to provide you with helpful and accurate information. Remember, I'm here to help and support you in any way I can, while ensuring that my responses are respectful, safe, and socially unbiased.\n",
      "{'model_name': 'meta-llama/llama-2-70b-chat', 'token_usage': {'prompt_tokens': 148, 'completion_tokens': 85, 'total_tokens': 233}}\n",
      "{'meta': {'conversation_id': '9debc536-f412-462e-9698-d59e7b63e294', 'created_at': datetime.datetime(2024, 5, 6, 11, 26, 52, 110000, tzinfo=TzInfo(UTC)), 'id': 'ca29b2cb-27d4-47ab-80e1-2257e5722132', 'model_id': 'meta-llama/llama-2-70b-chat'}, 'token_usage': {'prompt_tokens': 148, 'completion_tokens': 85, 'total_tokens': 233}, 'generated_token_count': 85, 'input_token_count': 148, 'input_tokens': [{'text': '<s>'}, {'text': '▁['}, {'text': 'INST'}, {'text': ']'}, {'text': '▁<<'}, {'text': 'SY'}, {'text': 'S'}, {'text': '>>'}, {'text': '<0x0A>'}, {'text': 'You'}, {'text': '▁are'}, {'text': '▁a'}, {'text': '▁helpful'}, {'text': ','}, {'text': '▁respect'}, {'text': 'ful'}, {'text': '▁and'}, {'text': '▁honest'}, {'text': '▁assistant'}, {'text': '.'}, {'text': '<0x0A>'}, {'text': 'Al'}, {'text': 'ways'}, {'text': '▁answer'}, {'text': '▁as'}, {'text': '▁help'}, {'text': 'fully'}, {'text': '▁as'}, {'text': '▁possible'}, {'text': ','}, {'text': '▁while'}, {'text': '▁being'}, {'text': '▁safe'}, {'text': '.'}, {'text': '<0x0A>'}, {'text': 'Your'}, {'text': '▁answers'}, {'text': '▁should'}, {'text': '▁not'}, {'text': '▁include'}, {'text': '▁any'}, {'text': '▁harm'}, {'text': 'ful'}, {'text': ','}, {'text': '▁un'}, {'text': 'eth'}, {'text': 'ical'}, {'text': ','}, {'text': '▁rac'}, {'text': 'ist'}, {'text': ','}, {'text': '▁sex'}, {'text': 'ist'}, {'text': ','}, {'text': '▁to'}, {'text': 'xic'}, {'text': ','}, {'text': '▁dangerous'}, {'text': ','}, {'text': '▁or'}, {'text': '▁illegal'}, {'text': '▁content'}, {'text': '.'}, {'text': '<0x0A>'}, {'text': 'Please'}, {'text': '▁ensure'}, {'text': '▁that'}, {'text': '▁your'}, {'text': '▁responses'}, {'text': '▁are'}, {'text': '▁soci'}, {'text': 'ally'}, {'text': '▁un'}, {'text': 'bi'}, {'text': 'ased'}, {'text': '▁and'}, {'text': '▁positive'}, {'text': '▁in'}, {'text': '▁nature'}, {'text': '.'}, {'text': '▁If'}, {'text': '▁a'}, {'text': '▁question'}, {'text': '▁does'}, {'text': '▁not'}, {'text': '▁make'}, {'text': '<0x0A>'}, {'text': 'any'}, {'text': '▁sense'}, {'text': ','}, {'text': '▁or'}, {'text': '▁is'}, {'text': '▁not'}, {'text': '▁fact'}, {'text': 'ually'}, {'text': '▁coh'}, {'text': 'er'}, {'text': 'ent'}, {'text': ','}, {'text': '▁explain'}, {'text': '▁why'}, {'text': '▁instead'}, {'text': '▁of'}, {'text': '▁answering'}, {'text': '▁something'}, {'text': '▁incorrectly'}, {'text': '.'}, {'text': '<0x0A>'}, {'text': 'If'}, {'text': '▁you'}, {'text': '▁don'}, {'text': \"'\"}, {'text': 't'}, {'text': '▁know'}, {'text': '▁the'}, {'text': '▁answer'}, {'text': '▁to'}, {'text': '▁a'}, {'text': '▁question'}, {'text': ','}, {'text': '▁please'}, {'text': '▁don'}, {'text': \"'\"}, {'text': 't'}, {'text': '▁share'}, {'text': '▁false'}, {'text': '▁information'}, {'text': '.'}, {'text': '<0x0A>'}, {'text': '<0x0A>'}, {'text': '<'}, {'text': '</'}, {'text': 'SY'}, {'text': 'S'}, {'text': '>>'}, {'text': '<0x0A>'}, {'text': '<0x0A>'}, {'text': 'Hi'}, {'text': ','}, {'text': '▁My'}, {'text': '▁name'}, {'text': '▁is'}, {'text': '▁Sa'}, {'text': 'ima'}, {'text': '▁['}, {'text': '/'}, {'text': 'INST'}, {'text': ']'}], 'moderations': {'hap': [{'flagged': True, 'position': {'end': 51, 'start': 0}, 'score': 0.12531322240829468, 'success': True}, {'flagged': True, 'position': {'end': 109, 'start': 52}, 'score': 0.12724095582962036, 'success': True}, {'flagged': True, 'position': {'end': 219, 'start': 110}, 'score': 0.5164276361465454, 'success': True}, {'flagged': True, 'position': {'end': 299, 'start': 220}, 'score': 0.12689636647701263, 'success': True}, {'flagged': True, 'position': {'end': 327, 'start': 300}, 'score': 0.13126897811889648, 'success': True}, {'flagged': True, 'position': {'end': 424, 'start': 328}, 'score': 0.13433726131916046, 'success': True}, {'flagged': True, 'position': {'end': 506, 'start': 425}, 'score': 0.1310122311115265, 'success': True}, {'flagged': True, 'position': {'end': 526, 'start': 507}, 'score': 0.12734292447566986, 'success': True}]}, 'seed': 4046048712.0, 'stop_reason': 'eos_token', 'moderation': {'hap': [{'flagged': True, 'position': {'end': 51, 'start': 0}, 'score': 0.12531322240829468, 'success': True}, {'flagged': True, 'position': {'end': 109, 'start': 52}, 'score': 0.12724095582962036, 'success': True}, {'flagged': True, 'position': {'end': 219, 'start': 110}, 'score': 0.5164276361465454, 'success': True}, {'flagged': True, 'position': {'end': 299, 'start': 220}, 'score': 0.12689636647701263, 'success': True}, {'flagged': True, 'position': {'end': 327, 'start': 300}, 'score': 0.13126897811889648, 'success': True}, {'flagged': True, 'position': {'end': 424, 'start': 328}, 'score': 0.13433726131916046, 'success': True}, {'flagged': True, 'position': {'end': 506, 'start': 425}, 'score': 0.1310122311115265, 'success': True}, {'flagged': True, 'position': {'end': 526, 'start': 507}, 'score': 0.12734292447566986, 'success': True}]}}\n",
      "\n",
      "===================== Continue conversation with langchain =====================\n",
      "\n",
      "Request: I would like you to know that I really like eating biryani\n",
      "Response:   That's great! Biryani is a popular and delicious dish that originated in South Asia. It's made with a mixture of rice, spices, and meat or vegetables, and can be prepared in many different ways. Do you have a favorite type of biryani or a special recipe that you enjoy?\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "from genai import Client, Credentials\n",
    "from genai.extensions.langchain.chat_llm import LangChainChatInterface\n",
    "from genai.schema import (\n",
    "    DecodingMethod,\n",
    "    ModerationHAP,\n",
    "    ModerationHAPInput,\n",
    "    ModerationParameters,\n",
    "    TextGenerationParameters,\n",
    "    TextGenerationReturnOptions,\n",
    ")\n",
    "\n",
    "# make sure you have a .env file under genai root with\n",
    "# GENAI_KEY=<your-genai-key>\n",
    "# GENAI_API=<genai-api-endpoint> (optional) DEFAULT_API = \"https://bam-api.res.ibm.com\"\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def heading(text: str) -> str:\n",
    "    \"\"\"Helper function for centering text.\"\"\"\n",
    "    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n",
    "\n",
    "\n",
    "llm = LangChainChatInterface(\n",
    "    client=Client(credentials=Credentials.from_env()),\n",
    "    model_id=\"meta-llama/llama-2-70b-chat\",\n",
    "    parameters=TextGenerationParameters(\n",
    "        decoding_method=DecodingMethod.SAMPLE,\n",
    "        max_new_tokens=100,\n",
    "        min_new_tokens=10,\n",
    "        temperature=0.5,\n",
    "        top_k=50,\n",
    "        top_p=1,\n",
    "        return_options=TextGenerationReturnOptions(input_text=False, input_tokens=True),\n",
    "    ),\n",
    "    moderations=ModerationParameters(\n",
    "        # Threshold is set to very low level to flag everything (testing purposes)\n",
    "        # or set to True to enable HAP with default settings\n",
    "        hap=ModerationHAP(input=ModerationHAPInput(enabled=True, threshold=0.01))\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(heading(\"Start conversation with langchain\"))\n",
    "prompt = input(\"Enter your prompt: \")\n",
    "print(f\"Request: {prompt}\")\n",
    "result = llm.generate(\n",
    "    messages=[\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=\"\"\"You are a helpful, respectful and honest assistant.\n",
    "Always answer as helpfully as possible, while being safe.\n",
    "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
    "Please ensure that your responses are socially unbiased and positive in nature. If a question does not make\n",
    "any sense, or is not factually coherent, explain why instead of answering something incorrectly.\n",
    "If you don't know the answer to a question, please don't share false information.\n",
    "\"\"\",\n",
    "            ),\n",
    "            HumanMessage(content=prompt),\n",
    "        ]\n",
    "    ],\n",
    ")\n",
    "conversation_id = result.generations[0][0].generation_info[\"meta\"][\"conversation_id\"]\n",
    "print(f\"New conversation with ID '{conversation_id}' has been created!\")\n",
    "print(f\"Response: {result.generations[0][0].text}\")\n",
    "print(result.llm_output)\n",
    "print(result.generations[0][0].generation_info)\n",
    "\n",
    "print(heading(\"Continue conversation with langchain\"))\n",
    "prompt = input(\"Enter your next prompt: \")\n",
    "print(f\"Request: {prompt}\")\n",
    "result = llm.generate(\n",
    "    messages=[[HumanMessage(content=prompt)]], conversation_id=conversation_id, use_conversation_parameters=True\n",
    ")\n",
    "print(f\"Response: {result.generations[0][0].text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running in loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================== Start conversation with langchain =======================\n",
      "\n",
      "Request: Hi I am Saima, I like to eat Biryani\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "LLMChain.generate() got an unexpected keyword argument 'messages'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequest: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 59\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mconversation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m                \u001b[49m\u001b[43mSystemMessage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[38;5;124;43mYou are a helpful, respectful and honest assistant.\u001b[39;49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;43mAlways answer as helpfully as possible, while being safe.\u001b[39;49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;43mYour answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\u001b[39;49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;43mPlease ensure that your responses are socially unbiased and positive in nature. If a question does not make\u001b[39;49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;43many sense, or is not factually coherent, explain why instead of answering something incorrectly.\u001b[39;49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;43mIf you don\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mt know the answer to a question, please don\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mt share false information.\u001b[39;49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m                \u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     conversation_id \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mgeneration_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconversation_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew conversation with ID \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversation_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has been created!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: LLMChain.generate() got an unexpected keyword argument 'messages'"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "from genai import Client, Credentials\n",
    "from genai.extensions.langchain.chat_llm import LangChainChatInterface\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from genai.schema import (\n",
    "    DecodingMethod,\n",
    "    ModerationHAP,\n",
    "    ModerationHAPInput,\n",
    "    ModerationParameters,\n",
    "    TextGenerationParameters,\n",
    "    TextGenerationReturnOptions,\n",
    ")\n",
    "\n",
    "# make sure you have a .env file under genai root with\n",
    "# GENAI_KEY=<your-genai-key>\n",
    "# GENAI_API=<genai-api-endpoint> (optional) DEFAULT_API = \"https://bam-api.res.ibm.com\"\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def heading(text: str) -> str:\n",
    "    \"\"\"Helper function for centering text.\"\"\"\n",
    "    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n",
    "\n",
    "\n",
    "llm = LangChainChatInterface(\n",
    "    client=Client(credentials=Credentials.from_env()),\n",
    "    model_id=\"meta-llama/llama-2-70b-chat\",\n",
    "    parameters=TextGenerationParameters(\n",
    "        decoding_method=DecodingMethod.SAMPLE,\n",
    "        max_new_tokens=100,\n",
    "        min_new_tokens=10,\n",
    "        temperature=0.5,\n",
    "        top_k=50,\n",
    "        top_p=1,\n",
    "        return_options=TextGenerationReturnOptions(input_text=False, input_tokens=True),\n",
    "    ),\n",
    "    moderations=ModerationParameters(\n",
    "        # Threshold is set to very low level to flag everything (testing purposes)\n",
    "        # or set to True to enable HAP with default settings\n",
    "        hap=ModerationHAP(input=ModerationHAPInput(enabled=True, threshold=0.01))\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(heading(\"Start conversation with langchain\"))\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory(),\n",
    "    verbose=True\n",
    ")\n",
    "while True:\n",
    "    prompt = input(\"Enter your prompt (or type 'exit' to end): \")\n",
    "    if prompt.lower() == 'exit':\n",
    "        break\n",
    "\n",
    "    print(f\"Request: {prompt}\")\n",
    "    result = conversation.generate(\n",
    "        messages=[\n",
    "            [\n",
    "                SystemMessage(\n",
    "                    content=\"\"\"You are a helpful, respectful and honest assistant.\n",
    "Always answer as helpfully as possible, while being safe.\n",
    "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
    "Please ensure that your responses are socially unbiased and positive in nature. If a question does not make\n",
    "any sense, or is not factually coherent, explain why instead of answering something incorrectly.\n",
    "If you don't know the answer to a question, please don't share false information.\n",
    "\"\"\",\n",
    "                ),\n",
    "                HumanMessage(content=prompt),\n",
    "            ]\n",
    "        ],\n",
    "    )\n",
    "    conversation_id = result.generations[0][0].generation_info[\"meta\"][\"conversation_id\"]\n",
    "    print(f\"New conversation with ID '{conversation_id}' has been created!\")\n",
    "    print(f\"Response: {result.generations[0][0].text}\")\n",
    "    print(result.llm_output)\n",
    "    print(result.generations[0][0].generation_info)\n",
    "\n",
    "print(heading(\"Conversation with langchain ended.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
