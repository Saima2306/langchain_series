{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# from dotenv import load_dotenv\n",
    "from genai.credentials import Credentials\n",
    "credentials = Credentials(api_key=\"pak-8prwoIymlhiNFNhxydsvEL8pl2VnQi_84sy03fvyLC0\", api_endpoint=\"https://bam-api.res.ibm.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GENAI_KEY\"] = \"pak-8prwoIymlhiNFNhxydsvEL8pl2VnQi_84sy03fvyLC0\"\n",
    "os.environ[\"GENAI_API\"] = \"https://bam-api.res.ibm.com/\"\n",
    "\n",
    "from genai.credentials import Credentials\n",
    "from genai.client import Client\n",
    "credentials = Credentials.from_env()\n",
    "client = Client(credentials=Credentials.from_env())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "from langchain.tools import Tool\n",
    "from genai.client import Client\n",
    "from genai.credentials import Credentials\n",
    "from genai.schema import (\n",
    "    DecodingMethod,\n",
    "    HumanMessage,\n",
    "    ModerationHAP,\n",
    "    ModerationHAPInput,\n",
    "    ModerationHAPOutput,\n",
    "    ModerationParameters,\n",
    "    SystemMessage,\n",
    "    TextGenerationParameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = TextGenerationParameters(\n",
    "    decoding_method=DecodingMethod.SAMPLE,\n",
    "    max_new_tokens=128,\n",
    "    min_new_tokens=30,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/llama-2-70b-chat\"\n",
    "prompt = \"What is NLP and how it has evolved over the years?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation ID: 5f9788df-26d0-4280-bece-5d0ab464cd78\n",
      "Request: What is NLP and how it has evolved over the years?\n",
      "Response:   NLP stands for Natural Language Processing, which is a subfield of artificial intelligence (AI) that deals with the interaction between computers and humans in natural language. It involves the development of algorithms and statistical models that enable computers to process, understand, and generate natural language data.\n",
      "\n",
      "NLP has evolved significantly over the years, driven by advances in machine learning, computational power, and the availability of large datasets. Here's a brief overview of the major milestones in the evolution of NLP:\n",
      "\n",
      "1. Early Beginnings (1950s-1960s\n",
      "Request: How can I start?\n",
      "Response:   To get started with NLP, you can follow these steps:\n",
      "\n",
      "1. Learn the basics of programming: NLP involves writing code to process and analyze natural language data. Therefore, it's essential to have a good understanding of programming concepts, data structures, and algorithms. Python is a popular language used in NLP, so you can start by learning Python.\n",
      "2. Learn the basics of NLP: Start by learning the basic concepts of NLP, such as tokenization, stemming, lemmatization, parsing, and semantic reasoning. You can find many online resources that provide tutorials and\n"
     ]
    }
   ],
   "source": [
    "response = client.text.chat.create(\n",
    "    model_id=model_id,\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"\"\"You are a helpful, respectful and honest assistant.\n",
    "Always answer as helpfully as possible, while being safe.\n",
    "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
    "Please ensure that your responses are socially unbiased and positive in nature. If a question does not make\n",
    "any sense, or is not factually coherent, explain why instead of answering something incorrectly.\n",
    "If you don't know the answer to a question, please don't share false information.\n",
    "\"\"\",\n",
    "        ),\n",
    "        HumanMessage(content=prompt),\n",
    "    ],\n",
    "    parameters=parameters,\n",
    ")\n",
    "conversation_id = response.conversation_id\n",
    "print(f\"Conversation ID: {conversation_id}\")\n",
    "print(f\"Request: {prompt}\")\n",
    "print(f\"Response: {response.results[0].generated_text}\")\n",
    "prompt = \"How can I start?\"\n",
    "response = client.text.chat.create(\n",
    "    messages=[HumanMessage(content=prompt)],\n",
    "    moderations=ModerationParameters(\n",
    "        hap=ModerationHAP(\n",
    "            input=ModerationHAPInput(enabled=True, threshold=0.8),\n",
    "            output=ModerationHAPOutput(enabled=True, threshold=0.8),\n",
    "        )\n",
    "    ),\n",
    "    conversation_id=conversation_id,\n",
    "    use_conversation_parameters=True,\n",
    ")\n",
    "print(f\"Request: {prompt}\")\n",
    "print(f\"Response: {response.results[0].generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m  Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"GetWordLength\",\n",
      "  \"action_input\": \"educA\"\n",
      "}\n",
      "```\n",
      "Observation\u001b[0m\u001b[36;1m\u001b[1;3m5\u001b[0m\u001b[32;1m\u001b[1;3m  Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"The word 'educA' has 5 letters.\"\n",
      "}\n",
      "```\n",
      "Observation\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m  {\n",
      "\"action\": \"GetWordLength\",\n",
      "\"action_input\": \"educA\"\n",
      "}\n",
      "\n",
      "Observation\u001b[0m\u001b[36;1m\u001b[1;3m5\u001b[0m\u001b[32;1m\u001b[1;3m  Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"GetWordLength\",\n",
      "  \"action_input\": \"education\"\n",
      "}\n",
      "```\n",
      "Observation\u001b[0m\u001b[36;1m\u001b[1;3m9\u001b[0m\u001b[32;1m\u001b[1;3m  Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"The word 'education' has 9 letters.\"\n",
      "}\n",
      "```\n",
      "Observation\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"That's not a real word, can you tell me a valid word?\",\n",
       " 'history': \"Human: How many letters are in the word educa?\\nAI: The word 'educA' has 5 letters.\",\n",
       " 'output': \"The word 'education' has 9 letters.\"}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.format_scratchpad import format_log_to_str\n",
    "from langchain.agents.output_parsers import JSONAgentOutputParser\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.tools.render import render_text_description_and_args\n",
    "from langchain_core.callbacks import CallbackManagerForToolRun\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.tools import BaseTool\n",
    "\n",
    "from genai import Client, Credentials\n",
    "from genai.extensions.langchain import LangChainChatInterface\n",
    "from genai.schema import TextGenerationParameters\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class WordLengthTool(BaseTool):\n",
    "    name = \"GetWordLength\"\n",
    "    description = \"Returns the length of a word.\"\n",
    "\n",
    "    def _run(self, word: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> int:\n",
    "        return len(word)\n",
    "\n",
    "\n",
    "tools: list[BaseTool] = [WordLengthTool()]\n",
    "\n",
    "system_prompt = \"\"\"Respond to the human as helpfully and accurately as possible. You have access to the following tools:\n",
    "{tools}\n",
    "Use a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\n",
    "Valid \"action\" values: \"Final Answer\" or {tool_names}\n",
    "Provide only ONE action per $JSON_BLOB, as shown:\n",
    "```\n",
    "{{\n",
    "  \"action\": $TOOL_NAME,\n",
    "  \"action_input\": $INPUT\n",
    "}}\n",
    "```\n",
    "Follow this format:\n",
    "Question: input question to answer\n",
    "Thought: consider previous and subsequent steps\n",
    "Action:\n",
    "```\n",
    "$JSON_BLOB\n",
    "```\n",
    "Observation: action result\n",
    "... (repeat Thought/Action/Observation N times)\n",
    "Thought: I know what to respond\n",
    "Action:\n",
    "```\n",
    "{{\n",
    "  \"action\": \"Final Answer\",\n",
    "  \"action_input\": \"Final response to human\"\n",
    "}}\n",
    "Begin! Reminder to ALWAYS respond with a valid json blob of a single action.\n",
    "Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation\"\"\"\n",
    "human_prompt = \"\"\"{input}\n",
    "{agent_scratchpad}\n",
    "(reminder to respond in a JSON blob no matter what)\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
    "        (\"human\", human_prompt),\n",
    "    ]\n",
    ")\n",
    "\n",
    "client = Client(credentials=Credentials.from_env())\n",
    "llm = LangChainChatInterface(\n",
    "    client=client,\n",
    "    model_id=\"meta-llama/llama-2-70b-chat\",\n",
    "    parameters=TextGenerationParameters(\n",
    "        max_new_tokens=250, min_new_tokens=20, temperature=0, stop_sequences=[\"\\nObservation\"]\n",
    "    ),\n",
    ")\n",
    "prompt = prompt.partial(\n",
    "    # get tools with their descriptions and args in plain text\n",
    "    tools=render_text_description_and_args(list(tools)),\n",
    "    tool_names=\", \".join([t.name for t in tools]),\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "agent = (\n",
    "    RunnablePassthrough.assign(\n",
    "        # format the agent's scratchpad to a string\n",
    "        agent_scratchpad=lambda x: format_log_to_str(x[\"intermediate_steps\"]),\n",
    "        # pass the memory as the chat history\n",
    "        chat_history=lambda x: memory.chat_memory.messages,\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    "    | JSONAgentOutputParser()\n",
    ")\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, handle_parsing_errors=True, verbose=True, memory=memory)\n",
    "\n",
    "agent_executor.invoke({\"input\": \"How many letters are in the word educa?\"})\n",
    "agent_executor.invoke({\"input\": \"That's not a real word, can you tell me a valid word?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================== Start conversation with langchain =======================\n",
      "\n",
      "Request: Describe what is Python in one sentence.\n",
      "New conversation with ID '1032015e-a281-4b65-85c8-5c0e0cd068fb' has been created!\n",
      "Response:   Python is a popular, versatile, and user-friendly programming language that is widely used for various purposes such as web development, data analysis, artificial intelligence, scientific computing, and more.\n",
      "{'model_name': 'meta-llama/llama-2-70b-chat', 'token_usage': {'prompt_tokens': 150, 'completion_tokens': 41, 'total_tokens': 191}}\n",
      "{'meta': {'conversation_id': '1032015e-a281-4b65-85c8-5c0e0cd068fb', 'created_at': datetime.datetime(2024, 5, 5, 17, 48, 18, 614000, tzinfo=TzInfo(UTC)), 'id': 'db02e8c2-1628-4a39-a8e0-a73558e03cda', 'model_id': 'meta-llama/llama-2-70b-chat'}, 'token_usage': {'prompt_tokens': 150, 'completion_tokens': 41, 'total_tokens': 191}, 'generated_token_count': 41, 'input_token_count': 150, 'input_tokens': [{'text': '<s>'}, {'text': '▁['}, {'text': 'INST'}, {'text': ']'}, {'text': '▁<<'}, {'text': 'SY'}, {'text': 'S'}, {'text': '>>'}, {'text': '<0x0A>'}, {'text': 'You'}, {'text': '▁are'}, {'text': '▁a'}, {'text': '▁helpful'}, {'text': ','}, {'text': '▁respect'}, {'text': 'ful'}, {'text': '▁and'}, {'text': '▁honest'}, {'text': '▁assistant'}, {'text': '.'}, {'text': '<0x0A>'}, {'text': 'Al'}, {'text': 'ways'}, {'text': '▁answer'}, {'text': '▁as'}, {'text': '▁help'}, {'text': 'fully'}, {'text': '▁as'}, {'text': '▁possible'}, {'text': ','}, {'text': '▁while'}, {'text': '▁being'}, {'text': '▁safe'}, {'text': '.'}, {'text': '<0x0A>'}, {'text': 'Your'}, {'text': '▁answers'}, {'text': '▁should'}, {'text': '▁not'}, {'text': '▁include'}, {'text': '▁any'}, {'text': '▁harm'}, {'text': 'ful'}, {'text': ','}, {'text': '▁un'}, {'text': 'eth'}, {'text': 'ical'}, {'text': ','}, {'text': '▁rac'}, {'text': 'ist'}, {'text': ','}, {'text': '▁sex'}, {'text': 'ist'}, {'text': ','}, {'text': '▁to'}, {'text': 'xic'}, {'text': ','}, {'text': '▁dangerous'}, {'text': ','}, {'text': '▁or'}, {'text': '▁illegal'}, {'text': '▁content'}, {'text': '.'}, {'text': '<0x0A>'}, {'text': 'Please'}, {'text': '▁ensure'}, {'text': '▁that'}, {'text': '▁your'}, {'text': '▁responses'}, {'text': '▁are'}, {'text': '▁soci'}, {'text': 'ally'}, {'text': '▁un'}, {'text': 'bi'}, {'text': 'ased'}, {'text': '▁and'}, {'text': '▁positive'}, {'text': '▁in'}, {'text': '▁nature'}, {'text': '.'}, {'text': '▁If'}, {'text': '▁a'}, {'text': '▁question'}, {'text': '▁does'}, {'text': '▁not'}, {'text': '▁make'}, {'text': '<0x0A>'}, {'text': 'any'}, {'text': '▁sense'}, {'text': ','}, {'text': '▁or'}, {'text': '▁is'}, {'text': '▁not'}, {'text': '▁fact'}, {'text': 'ually'}, {'text': '▁coh'}, {'text': 'er'}, {'text': 'ent'}, {'text': ','}, {'text': '▁explain'}, {'text': '▁why'}, {'text': '▁instead'}, {'text': '▁of'}, {'text': '▁answering'}, {'text': '▁something'}, {'text': '▁incorrectly'}, {'text': '.'}, {'text': '<0x0A>'}, {'text': 'If'}, {'text': '▁you'}, {'text': '▁don'}, {'text': \"'\"}, {'text': 't'}, {'text': '▁know'}, {'text': '▁the'}, {'text': '▁answer'}, {'text': '▁to'}, {'text': '▁a'}, {'text': '▁question'}, {'text': ','}, {'text': '▁please'}, {'text': '▁don'}, {'text': \"'\"}, {'text': 't'}, {'text': '▁share'}, {'text': '▁false'}, {'text': '▁information'}, {'text': '.'}, {'text': '<0x0A>'}, {'text': '<0x0A>'}, {'text': '<'}, {'text': '</'}, {'text': 'SY'}, {'text': 'S'}, {'text': '>>'}, {'text': '<0x0A>'}, {'text': '<0x0A>'}, {'text': 'Des'}, {'text': 'cribe'}, {'text': '▁what'}, {'text': '▁is'}, {'text': '▁Python'}, {'text': '▁in'}, {'text': '▁one'}, {'text': '▁sentence'}, {'text': '.'}, {'text': '▁['}, {'text': '/'}, {'text': 'INST'}, {'text': ']'}], 'moderations': {'hap': [{'flagged': True, 'position': {'end': 51, 'start': 0}, 'score': 0.12531322240829468, 'success': True}, {'flagged': True, 'position': {'end': 109, 'start': 52}, 'score': 0.12724095582962036, 'success': True}, {'flagged': True, 'position': {'end': 219, 'start': 110}, 'score': 0.5164276361465454, 'success': True}, {'flagged': True, 'position': {'end': 299, 'start': 220}, 'score': 0.12689636647701263, 'success': True}, {'flagged': True, 'position': {'end': 327, 'start': 300}, 'score': 0.13126897811889648, 'success': True}, {'flagged': True, 'position': {'end': 424, 'start': 328}, 'score': 0.13433726131916046, 'success': True}, {'flagged': True, 'position': {'end': 506, 'start': 425}, 'score': 0.1310122311115265, 'success': True}, {'flagged': True, 'position': {'end': 546, 'start': 507}, 'score': 0.14043477177619934, 'success': True}]}, 'seed': 891549058.0, 'stop_reason': 'eos_token', 'moderation': {'hap': [{'flagged': True, 'position': {'end': 51, 'start': 0}, 'score': 0.12531322240829468, 'success': True}, {'flagged': True, 'position': {'end': 109, 'start': 52}, 'score': 0.12724095582962036, 'success': True}, {'flagged': True, 'position': {'end': 219, 'start': 110}, 'score': 0.5164276361465454, 'success': True}, {'flagged': True, 'position': {'end': 299, 'start': 220}, 'score': 0.12689636647701263, 'success': True}, {'flagged': True, 'position': {'end': 327, 'start': 300}, 'score': 0.13126897811889648, 'success': True}, {'flagged': True, 'position': {'end': 424, 'start': 328}, 'score': 0.13433726131916046, 'success': True}, {'flagged': True, 'position': {'end': 506, 'start': 425}, 'score': 0.1310122311115265, 'success': True}, {'flagged': True, 'position': {'end': 546, 'start': 507}, 'score': 0.14043477177619934, 'success': True}]}}\n",
      "\n",
      "===================== Continue conversation with langchain =====================\n",
      "\n",
      "Request: Show me some simple code example.\n",
      "Response:   Sure, here's a simple Python code example that prints a \"Hello, World!\" message to the console:\n",
      "```\n",
      "print(\"Hello, World!\")\n",
      "```\n",
      "This code uses the `print()` function to display the string \"Hello, World!\" on the screen. When you run this code, you should see the message printed in the console or terminal window.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "from genai import Client, Credentials\n",
    "from genai.extensions.langchain.chat_llm import LangChainChatInterface\n",
    "from genai.schema import (\n",
    "    DecodingMethod,\n",
    "    ModerationHAP,\n",
    "    ModerationHAPInput,\n",
    "    ModerationParameters,\n",
    "    TextGenerationParameters,\n",
    "    TextGenerationReturnOptions,\n",
    ")\n",
    "\n",
    "# make sure you have a .env file under genai root with\n",
    "# GENAI_KEY=<your-genai-key>\n",
    "# GENAI_API=<genai-api-endpoint> (optional) DEFAULT_API = \"https://bam-api.res.ibm.com\"\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def heading(text: str) -> str:\n",
    "    \"\"\"Helper function for centering text.\"\"\"\n",
    "    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n",
    "\n",
    "\n",
    "llm = LangChainChatInterface(\n",
    "    client=Client(credentials=Credentials.from_env()),\n",
    "    model_id=\"meta-llama/llama-2-70b-chat\",\n",
    "    parameters=TextGenerationParameters(\n",
    "        decoding_method=DecodingMethod.SAMPLE,\n",
    "        max_new_tokens=100,\n",
    "        min_new_tokens=10,\n",
    "        temperature=0.5,\n",
    "        top_k=50,\n",
    "        top_p=1,\n",
    "        return_options=TextGenerationReturnOptions(input_text=False, input_tokens=True),\n",
    "    ),\n",
    "    moderations=ModerationParameters(\n",
    "        # Threshold is set to very low level to flag everything (testing purposes)\n",
    "        # or set to True to enable HAP with default settings\n",
    "        hap=ModerationHAP(input=ModerationHAPInput(enabled=True, threshold=0.01))\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(heading(\"Start conversation with langchain\"))\n",
    "prompt = \"Describe what is Python in one sentence.\"\n",
    "print(f\"Request: {prompt}\")\n",
    "result = llm.generate(\n",
    "    messages=[\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=\"\"\"You are a helpful, respectful and honest assistant.\n",
    "Always answer as helpfully as possible, while being safe.\n",
    "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
    "Please ensure that your responses are socially unbiased and positive in nature. If a question does not make\n",
    "any sense, or is not factually coherent, explain why instead of answering something incorrectly.\n",
    "If you don't know the answer to a question, please don't share false information.\n",
    "\"\"\",\n",
    "            ),\n",
    "            HumanMessage(content=prompt),\n",
    "        ]\n",
    "    ],\n",
    ")\n",
    "conversation_id = result.generations[0][0].generation_info[\"meta\"][\"conversation_id\"]\n",
    "print(f\"New conversation with ID '{conversation_id}' has been created!\")\n",
    "print(f\"Response: {result.generations[0][0].text}\")\n",
    "print(result.llm_output)\n",
    "print(result.generations[0][0].generation_info)\n",
    "\n",
    "print(heading(\"Continue conversation with langchain\"))\n",
    "prompt = \"Show me some simple code example.\"\n",
    "print(f\"Request: {prompt}\")\n",
    "result = llm.generate(\n",
    "    messages=[[HumanMessage(content=prompt)]], conversation_id=conversation_id, use_conversation_parameters=True\n",
    ")\n",
    "print(f\"Response: {result.generations[0][0].text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1714932570'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "session_id = str(int(time.time()))\n",
    "session_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3b2d0e11-5055-4ad7-b17c-6626bb193fbb'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "session_id = str(uuid.uuid4())\n",
    "session_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dotenv import load_dotenv\n",
    "# from langchain_core.messages import HumanMessage, SystemMessage\n",
    "# from genai import Client, Credentials\n",
    "# from genai.extensions.langchain.chat_llm import LangChainChatInterface\n",
    "# from genai.extensions.langchain import LangChainInterface\n",
    "# from genai.schema import (\n",
    "#     DecodingMethod,\n",
    "#     ModerationHAP,\n",
    "#     ModerationHAPInput,\n",
    "#     ModerationParameters,\n",
    "#     TextGenerationParameters,\n",
    "#     TextGenerationReturnOptions,\n",
    "# )\n",
    "# from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "# from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# # Load environment variables\n",
    "# load_dotenv()\n",
    "\n",
    "# def heading(text: str) -> str:\n",
    "#     \"\"\"Helper function for centering text.\"\"\"\n",
    "#     return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n",
    "\n",
    "# # Create the custom template prompt\n",
    "# client = Client(credentials=Credentials.from_env())\n",
    "# prompt_response = client.prompt.create(\n",
    "#     name=\"Recipe Generator Prompt\",\n",
    "#     model_id=\"google/flan-t5-xl\",\n",
    "#     input=\"Make a short recipe for {{meal}} (use bullet points)\",\n",
    "# )\n",
    "\n",
    "# # Create the LangChain Chat Interface with memory functionality\n",
    "# message_history = ChatMessageHistory()\n",
    "# agent_with_chat_history = RunnableWithMessageHistory(\n",
    "#     LangChainChatInterface(\n",
    "#         client=client,\n",
    "#         model_id=\"ibm/granite-13b-instruct-v2\",\n",
    "#         prompt_id=prompt_response.result.id,\n",
    "#         parameters=TextGenerationParameters(\n",
    "#             decoding_method=DecodingMethod.SAMPLE,\n",
    "#             max_new_tokens=100,\n",
    "#             min_new_tokens=10,\n",
    "#             temperature=0.5,\n",
    "#             top_k=50,\n",
    "#             top_p=1,\n",
    "#             return_options=TextGenerationReturnOptions(input_text=False, input_tokens=True),\n",
    "#         ),\n",
    "#         moderations=ModerationParameters(\n",
    "#             hap=ModerationHAP(input=ModerationHAPInput(enabled=True, threshold=0.01))\n",
    "#         ),\n",
    "#         data={\"meal\": \"Lasagne\"},\n",
    "#     ),\n",
    "#     lambda session_id: message_history,\n",
    "#     input_messages_key=\"input\",\n",
    "#     history_messages_key=\"chat_history\",\n",
    "# )\n",
    "\n",
    "# # Start a conversation with memory functionality\n",
    "# print(heading(\"Start conversation with langchain\"))\n",
    "# prompt = \"Describe what is Python in one sentence.\"\n",
    "# print(f\"Request: {prompt}\")\n",
    "# result = agent_with_chat_history.invoke(\n",
    "#     [\n",
    "#         SystemMessage(\n",
    "#             content=\"\"\"You are a helpful, respectful and honest assistant.\n",
    "# Always answer as helpfully as possible, while being safe.\n",
    "# Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
    "# Please ensure that your responses are socially unbiased and positive in nature. If a question does not make\n",
    "# any sense, or is not factually coherent, explain why instead of answering something incorrectly.\n",
    "# If you don't know the answer to a question, please don't share false information.\n",
    "# \"\"\",\n",
    "#             user_id=\"assistant\"\n",
    "#         ),\n",
    "#         HumanMessage(content=prompt, user_id=\"user\")\n",
    "#     ],\n",
    "#     {\"configurable\": {\"session_id\": session_id}}\n",
    "# )\n",
    "\n",
    "# conversation_id = result.generations[0][0].generation_info[\"meta\"][\"conversation_id\"]\n",
    "# print(f\"New conversation with ID '{conversation_id}' has been created!\")\n",
    "# print(f\"Response: {result.generations[0][0].text}\")\n",
    "# print(result.llm_output)\n",
    "# print(result.generations[0][0].generation_info)\n",
    "\n",
    "# # Continue the conversation with memory functionality\n",
    "# print(heading(\"Continue conversation with langchain\"))\n",
    "# prompt = \"Show me some simple code example.\"\n",
    "# print(f\"Request: {prompt}\")\n",
    "# result = agent_with_chat_history.invoke(\n",
    "#     {\"input\": [HumanMessage(content=prompt, user_id=\"user\")]},\n",
    "#     {\"configurable\": {\"session_id\": conversation_id}},\n",
    "#     use_conversation_parameters=True,\n",
    "# )\n",
    "# print(f\"Response: {result.generations[0][0].text}\")\n",
    "\n",
    "# # Delete the prompt if no longer needed\n",
    "# client.prompt.delete(prompt_response.result.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dotenv import load_dotenv\n",
    "# from langchain_core.messages import HumanMessage, SystemMessage\n",
    "# from genai import Client, Credentials\n",
    "# from genai.extensions.langchain.chat_llm import LangChainChatInterface\n",
    "# from genai.extensions.langchain import LangChainInterface\n",
    "# from genai.schema import (\n",
    "#     DecodingMethod,\n",
    "#     ModerationHAP,\n",
    "#     ModerationHAPInput,\n",
    "#     ModerationParameters,\n",
    "#     TextGenerationParameters,\n",
    "#     TextGenerationReturnOptions,\n",
    "# )\n",
    "# from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "# from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# # Load environment variables\n",
    "# load_dotenv()\n",
    "\n",
    "# def heading(text: str) -> str:\n",
    "#     \"\"\"Helper function for centering text.\"\"\"\n",
    "#     return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n",
    "\n",
    "# # Create the custom template prompt\n",
    "# client = Client(credentials=Credentials.from_env())\n",
    "# prompt_response = client.prompt.create(\n",
    "#     name=\"Recipe Generator Prompt\",\n",
    "#     model_id=\"google/flan-t5-xl\",\n",
    "#     input=\"Make a short recipe for {{meal}} (use bullet points)\",\n",
    "# )\n",
    "\n",
    "# # Create the LangChain Chat Interface with memory functionality\n",
    "# message_history = ChatMessageHistory()\n",
    "# agent_with_chat_history = RunnableWithMessageHistory(\n",
    "#     LangChainChatInterface(\n",
    "#         client=client,\n",
    "#         model_id=\"ibm/granite-13b-instruct-v2\",\n",
    "#         prompt_id=prompt_response.result.id,\n",
    "#         parameters=TextGenerationParameters(\n",
    "#             decoding_method=DecodingMethod.SAMPLE,\n",
    "#             max_new_tokens=100,\n",
    "#             min_new_tokens=10,\n",
    "#             temperature=0.5,\n",
    "#             top_k=50,\n",
    "#             top_p=1,\n",
    "#             return_options=TextGenerationReturnOptions(input_text=False, input_tokens=True),\n",
    "#         ),\n",
    "#         moderations=ModerationParameters(\n",
    "#             hap=ModerationHAP(input=ModerationHAPInput(enabled=True, threshold=0.01))\n",
    "#         ),\n",
    "#         data={\"meal\": \"Lasagne\"},\n",
    "#     ),\n",
    "#     lambda session_id: message_history,\n",
    "#     input_messages_key=\"input\",\n",
    "#     history_messages_key=\"chat_history\",\n",
    "# )\n",
    "\n",
    "# # Start a conversation with memory functionality\n",
    "# print(heading(\"Start conversation with langchain\"))\n",
    "# prompt = \"Describe what is Python in one sentence.\"\n",
    "# print(f\"Request: {prompt}\")\n",
    "# result = agent_with_chat_history.invoke(\n",
    "#     [\n",
    "#         SystemMessage(\n",
    "#             content=\"\"\"You are a helpful, respectful and honest assistant.\n",
    "# Always answer as helpfully as possible, while being safe.\n",
    "# Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
    "# Please ensure that your responses are socially unbiased and positive in nature. If a question does not make\n",
    "# any sense, or is not factually coherent, explain why instead of answering something incorrectly.\n",
    "# If you don't know the answer to a question, please don't share false information.\n",
    "# \"\"\",\n",
    "#             user_id=\"assistant\"\n",
    "#         ),\n",
    "#         HumanMessage(content=prompt, user_id=\"user\")\n",
    "#     ],\n",
    "#     {\"configurable\": {\"session_id\": session_id}}\n",
    "# )\n",
    "\n",
    "# conversation_id = result.generations[0][0].generation_info[\"meta\"][\"conversation_id\"]\n",
    "# print(f\"New conversation with ID '{conversation_id}' has been created!\")\n",
    "# print(f\"Response: {result.generations[0][0].text}\")\n",
    "# print(result.llm_output)\n",
    "# print(result.generations[0][0].generation_info)\n",
    "\n",
    "# # Continue the conversation with memory functionality\n",
    "# print(heading(\"Continue conversation with langchain\"))\n",
    "# prompt = \"Show me some simple code example.\"\n",
    "# print(f\"Request: {prompt}\")\n",
    "# result = agent_with_chat_history.invoke(\n",
    "#     {\"input\": [HumanMessage(content=prompt, user_id=\"user\")]},\n",
    "#     {\"configurable\": {\"session_id\": conversation_id}},\n",
    "#     use_conversation_parameters=True,\n",
    "# )\n",
    "# print(f\"Response: {result.generations[0][0].text}\")\n",
    "\n",
    "# # Delete the prompt if no longer needed\n",
    "# client.prompt.delete(prompt_response.result.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Agent using IBM SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m  Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"GetWordLength\",\n",
      "  \"action_input\": \"educA\"\n",
      "}\n",
      "```\n",
      "Observation\u001b[0m\u001b[36;1m\u001b[1;3m5\u001b[0m\u001b[32;1m\u001b[1;3m  Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"The word 'educA' has 5 letters.\"\n",
      "}\n",
      "```\n",
      "Observation\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m  {\n",
      "\"action\": \"GetWordLength\",\n",
      "\"action_input\": \"educA\"\n",
      "}\n",
      "\n",
      "Observation\u001b[0m\u001b[36;1m\u001b[1;3m5\u001b[0m\u001b[32;1m\u001b[1;3m  Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"GetWordLength\",\n",
      "  \"action_input\": \"education\"\n",
      "}\n",
      "```\n",
      "Observation\u001b[0m\u001b[36;1m\u001b[1;3m9\u001b[0m\u001b[32;1m\u001b[1;3m  Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"The word 'education' has 9 letters.\"\n",
      "}\n",
      "```\n",
      "Observation\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m  {\n",
      "\"action\": \"GetWordLength\",\n",
      "\"action_input\": \"education\"\n",
      "}\n",
      "\n",
      "{\n",
      "\"action\": \"GetWordLength\",\n",
      "\"action_input\": \"learning\"\n",
      "}\n",
      "\n",
      "{\n",
      "\"action\": \"GetWordLength\",\n",
      "\"action_input\": \"instruction\"\n",
      "}\n",
      "\n",
      "{\n",
      "\"action\": \"GetWordLength\",\n",
      "\"action_input\": \"teaching\"\n",
      "}\n",
      "\n",
      "{\n",
      "\"action\": \"GetWordLength\",\n",
      "\"action_input\": \"schooling\"\n",
      "}\n",
      "\n",
      "(reminder to respond directly)\n",
      "\n",
      "Thought: The human is asking for synonyms of the word \"education\".\n",
      "Action:\n",
      "{\n",
      "\"action\": \"Final Answer\",\n",
      "\"action_input\": \"Here are some synonyms of the word 'education': learning, instruction, teaching, schooling.\"\n",
      "}\n",
      "\n",
      "Observation\u001b[0m\u001b[36;1m\u001b[1;3m9\u001b[0m\u001b[32;1m\u001b[1;3m  {\n",
      "\"action\": \"GetWordLength\",\n",
      "\"action_input\": \"education\"\n",
      "}\n",
      "\n",
      "{\n",
      "\"action\": \"GetWordLength\",\n",
      "\"action_input\": \"learning\"\n",
      "}\n",
      "\n",
      "{\n",
      "\"action\": \"GetWordLength\",\n",
      "\"action_input\": \"instruction\"\n",
      "}\n",
      "\n",
      "{\n",
      "\"action\": \"GetWordLength\",\n",
      "\"action_input\": \"teaching\"\n",
      "}\n",
      "\n",
      "{\n",
      "\"action\": \"GetWordLength\",\n",
      "\"action_input\": \"schooling\"\n",
      "}\n",
      "\n",
      "{\n",
      "\"action\": \"Final Answer\",\n",
      "\"action_input\": \"Here are some synonyms of the word 'education': learning, instruction, teaching, schooling.\"\n",
      "}\n",
      "\n",
      "Observation\u001b[0m\u001b[36;1m\u001b[1;3m9\u001b[0m\u001b[32;1m\u001b[1;3m  {\n",
      "\"action\": \"Final Answer\",\n",
      "\"action_input\": \"Here are some synonyms of the word 'education': learning, instruction, teaching, schooling.\"\n",
      "}\n",
      "\n",
      "Observation\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Can you suggest me some synonyms of the word education?',\n",
       " 'history': \"Human: How many letters are in the word educa?\\nAI: The word 'educA' has 5 letters.\\nHuman: That's not a real word, can you tell me a valid word?\\nAI: The word 'education' has 9 letters.\",\n",
       " 'output': \"Here are some synonyms of the word 'education': learning, instruction, teaching, schooling.\"}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.format_scratchpad import format_log_to_str\n",
    "from langchain.agents.output_parsers import JSONAgentOutputParser\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.tools.render import render_text_description_and_args\n",
    "from langchain_core.callbacks import CallbackManagerForToolRun\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.tools import BaseTool\n",
    "\n",
    "from genai import Client, Credentials\n",
    "from genai.extensions.langchain import LangChainChatInterface\n",
    "from genai.schema import TextGenerationParameters\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class WordLengthTool(BaseTool):\n",
    "    name = \"GetWordLength\"\n",
    "    description = \"Returns the length of a word.\"\n",
    "\n",
    "    def _run(self, word: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> int:\n",
    "        return len(word)\n",
    "\n",
    "\n",
    "tools: list[BaseTool] = [WordLengthTool()]\n",
    "\n",
    "system_prompt = \"\"\"Respond to the human as helpfully and accurately as possible. You have access to the following tools:\n",
    "{tools}\n",
    "Use a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\n",
    "Valid \"action\" values: \"Final Answer\" or {tool_names}\n",
    "Provide only ONE action per $JSON_BLOB, as shown:\n",
    "```\n",
    "{{\n",
    "  \"action\": $TOOL_NAME,\n",
    "  \"action_input\": $INPUT\n",
    "}}\n",
    "```\n",
    "Follow this format:\n",
    "Question: input question to answer\n",
    "Thought: consider previous and subsequent steps\n",
    "Action:\n",
    "```\n",
    "$JSON_BLOB\n",
    "```\n",
    "Observation: action result\n",
    "... (repeat Thought/Action/Observation N times)\n",
    "Thought: I know what to respond\n",
    "Action:\n",
    "```\n",
    "{{\n",
    "  \"action\": \"Final Answer\",\n",
    "  \"action_input\": \"Final response to human\"\n",
    "}}\n",
    "Begin! Reminder to ALWAYS respond with a valid json blob of a single action.\n",
    "Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation\"\"\"\n",
    "human_prompt = \"\"\"{input}\n",
    "{agent_scratchpad}\n",
    "(reminder to respond in a JSON blob no matter what)\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
    "        (\"human\", human_prompt),\n",
    "    ]\n",
    ")\n",
    "\n",
    "client = Client(credentials=Credentials.from_env())\n",
    "llm = LangChainChatInterface(\n",
    "    client=client,\n",
    "    model_id=\"meta-llama/llama-2-70b-chat\",\n",
    "    parameters=TextGenerationParameters(\n",
    "        max_new_tokens=250, min_new_tokens=20, temperature=0, stop_sequences=[\"\\nObservation\"]\n",
    "    ),\n",
    ")\n",
    "prompt = prompt.partial(\n",
    "    # get tools with their descriptions and args in plain text\n",
    "    tools=render_text_description_and_args(list(tools)),\n",
    "    tool_names=\", \".join([t.name for t in tools]),\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "agent = (\n",
    "    RunnablePassthrough.assign(\n",
    "        # format the agent's scratchpad to a string\n",
    "        agent_scratchpad=lambda x: format_log_to_str(x[\"intermediate_steps\"]),\n",
    "        # pass the memory as the chat history\n",
    "        chat_history=lambda x: memory.chat_memory.messages,\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    "    | JSONAgentOutputParser()\n",
    ")\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, handle_parsing_errors=True, verbose=True, memory=memory)\n",
    "\n",
    "agent_executor.invoke({\"input\": \"How many letters are in the word educa?\"})\n",
    "agent_executor.invoke({\"input\": \"That's not a real word, can you tell me a valid word?\"})\n",
    "agent_executor.invoke({\"input\":\"Can you suggest me some synonyms of the word education?\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat a model using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================== Start conversation with langchain =======================\n",
      "\n",
      "Request: Describe what is Python in one sentence.\n",
      "New conversation with ID 'c82f8ef3-9710-4a86-9772-611d6b4d89a4' has been created!\n",
      "Response:   Python is a high-level, interpreted programming language that is widely used for various purposes such as web development, data analysis, artificial intelligence, and scientific computing, known for its simplicity, readability, and ease of use.\n",
      "{'model_name': 'meta-llama/llama-2-70b-chat', 'token_usage': {'prompt_tokens': 150, 'completion_tokens': 47, 'total_tokens': 197}}\n",
      "{'meta': {'conversation_id': 'c82f8ef3-9710-4a86-9772-611d6b4d89a4', 'created_at': datetime.datetime(2024, 5, 7, 2, 44, 6, 545000, tzinfo=TzInfo(UTC)), 'id': '088cc289-9550-4c3d-8e22-ec8cc4d817cd', 'model_id': 'meta-llama/llama-2-70b-chat'}, 'token_usage': {'prompt_tokens': 150, 'completion_tokens': 47, 'total_tokens': 197}, 'generated_token_count': 47, 'input_token_count': 150, 'input_tokens': [{'text': '<s>'}, {'text': '▁['}, {'text': 'INST'}, {'text': ']'}, {'text': '▁<<'}, {'text': 'SY'}, {'text': 'S'}, {'text': '>>'}, {'text': '<0x0A>'}, {'text': 'You'}, {'text': '▁are'}, {'text': '▁a'}, {'text': '▁helpful'}, {'text': ','}, {'text': '▁respect'}, {'text': 'ful'}, {'text': '▁and'}, {'text': '▁honest'}, {'text': '▁assistant'}, {'text': '.'}, {'text': '<0x0A>'}, {'text': 'Al'}, {'text': 'ways'}, {'text': '▁answer'}, {'text': '▁as'}, {'text': '▁help'}, {'text': 'fully'}, {'text': '▁as'}, {'text': '▁possible'}, {'text': ','}, {'text': '▁while'}, {'text': '▁being'}, {'text': '▁safe'}, {'text': '.'}, {'text': '<0x0A>'}, {'text': 'Your'}, {'text': '▁answers'}, {'text': '▁should'}, {'text': '▁not'}, {'text': '▁include'}, {'text': '▁any'}, {'text': '▁harm'}, {'text': 'ful'}, {'text': ','}, {'text': '▁un'}, {'text': 'eth'}, {'text': 'ical'}, {'text': ','}, {'text': '▁rac'}, {'text': 'ist'}, {'text': ','}, {'text': '▁sex'}, {'text': 'ist'}, {'text': ','}, {'text': '▁to'}, {'text': 'xic'}, {'text': ','}, {'text': '▁dangerous'}, {'text': ','}, {'text': '▁or'}, {'text': '▁illegal'}, {'text': '▁content'}, {'text': '.'}, {'text': '<0x0A>'}, {'text': 'Please'}, {'text': '▁ensure'}, {'text': '▁that'}, {'text': '▁your'}, {'text': '▁responses'}, {'text': '▁are'}, {'text': '▁soci'}, {'text': 'ally'}, {'text': '▁un'}, {'text': 'bi'}, {'text': 'ased'}, {'text': '▁and'}, {'text': '▁positive'}, {'text': '▁in'}, {'text': '▁nature'}, {'text': '.'}, {'text': '▁If'}, {'text': '▁a'}, {'text': '▁question'}, {'text': '▁does'}, {'text': '▁not'}, {'text': '▁make'}, {'text': '<0x0A>'}, {'text': 'any'}, {'text': '▁sense'}, {'text': ','}, {'text': '▁or'}, {'text': '▁is'}, {'text': '▁not'}, {'text': '▁fact'}, {'text': 'ually'}, {'text': '▁coh'}, {'text': 'er'}, {'text': 'ent'}, {'text': ','}, {'text': '▁explain'}, {'text': '▁why'}, {'text': '▁instead'}, {'text': '▁of'}, {'text': '▁answering'}, {'text': '▁something'}, {'text': '▁incorrectly'}, {'text': '.'}, {'text': '<0x0A>'}, {'text': 'If'}, {'text': '▁you'}, {'text': '▁don'}, {'text': \"'\"}, {'text': 't'}, {'text': '▁know'}, {'text': '▁the'}, {'text': '▁answer'}, {'text': '▁to'}, {'text': '▁a'}, {'text': '▁question'}, {'text': ','}, {'text': '▁please'}, {'text': '▁don'}, {'text': \"'\"}, {'text': 't'}, {'text': '▁share'}, {'text': '▁false'}, {'text': '▁information'}, {'text': '.'}, {'text': '<0x0A>'}, {'text': '<0x0A>'}, {'text': '<'}, {'text': '</'}, {'text': 'SY'}, {'text': 'S'}, {'text': '>>'}, {'text': '<0x0A>'}, {'text': '<0x0A>'}, {'text': 'Des'}, {'text': 'cribe'}, {'text': '▁what'}, {'text': '▁is'}, {'text': '▁Python'}, {'text': '▁in'}, {'text': '▁one'}, {'text': '▁sentence'}, {'text': '.'}, {'text': '▁['}, {'text': '/'}, {'text': 'INST'}, {'text': ']'}], 'moderations': {'hap': [{'flagged': True, 'position': {'end': 51, 'start': 0}, 'score': 0.12531322240829468, 'success': True}, {'flagged': True, 'position': {'end': 109, 'start': 52}, 'score': 0.12724095582962036, 'success': True}, {'flagged': True, 'position': {'end': 219, 'start': 110}, 'score': 0.5164276361465454, 'success': True}, {'flagged': True, 'position': {'end': 299, 'start': 220}, 'score': 0.12689636647701263, 'success': True}, {'flagged': True, 'position': {'end': 327, 'start': 300}, 'score': 0.13126897811889648, 'success': True}, {'flagged': True, 'position': {'end': 424, 'start': 328}, 'score': 0.13433726131916046, 'success': True}, {'flagged': True, 'position': {'end': 506, 'start': 425}, 'score': 0.1310122311115265, 'success': True}, {'flagged': True, 'position': {'end': 546, 'start': 507}, 'score': 0.14043477177619934, 'success': True}]}, 'seed': 884529298.0, 'stop_reason': 'eos_token', 'moderation': {'hap': [{'flagged': True, 'position': {'end': 51, 'start': 0}, 'score': 0.12531322240829468, 'success': True}, {'flagged': True, 'position': {'end': 109, 'start': 52}, 'score': 0.12724095582962036, 'success': True}, {'flagged': True, 'position': {'end': 219, 'start': 110}, 'score': 0.5164276361465454, 'success': True}, {'flagged': True, 'position': {'end': 299, 'start': 220}, 'score': 0.12689636647701263, 'success': True}, {'flagged': True, 'position': {'end': 327, 'start': 300}, 'score': 0.13126897811889648, 'success': True}, {'flagged': True, 'position': {'end': 424, 'start': 328}, 'score': 0.13433726131916046, 'success': True}, {'flagged': True, 'position': {'end': 506, 'start': 425}, 'score': 0.1310122311115265, 'success': True}, {'flagged': True, 'position': {'end': 546, 'start': 507}, 'score': 0.14043477177619934, 'success': True}]}}\n",
      "\n",
      "===================== Continue conversation with langchain =====================\n",
      "\n",
      "Request: Show me the example of code which can print fibonacci series.\n",
      "Response:   Sure! Here's an example of Python code that prints the Fibonacci series up to a given number:\n",
      "```\n",
      "def fibonacci(n):\n",
      "    a, b = 0, 1\n",
      "    for i in range(n):\n",
      "        print(b, end=\" \")\n",
      "        a, b = b, a + b\n",
      "    print()\n",
      "\n",
      "# Example usage:\n",
      "fibonacci(5)\n",
      "```\n",
      "This code defines a\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "from genai import Client, Credentials\n",
    "from genai.extensions.langchain.chat_llm import LangChainChatInterface\n",
    "from genai.schema import (\n",
    "    DecodingMethod,\n",
    "    ModerationHAP,\n",
    "    ModerationHAPInput,\n",
    "    ModerationParameters,\n",
    "    TextGenerationParameters,\n",
    "    TextGenerationReturnOptions,\n",
    ")\n",
    "\n",
    "# make sure you have a .env file under genai root with\n",
    "# GENAI_KEY=<your-genai-key>\n",
    "# GENAI_API=<genai-api-endpoint> (optional) DEFAULT_API = \"https://bam-api.res.ibm.com\"\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def heading(text: str) -> str:\n",
    "    \"\"\"Helper function for centering text.\"\"\"\n",
    "    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n",
    "\n",
    "\n",
    "llm = LangChainChatInterface(\n",
    "    client=Client(credentials=Credentials.from_env()),\n",
    "    model_id=\"meta-llama/llama-2-70b-chat\",\n",
    "    parameters=TextGenerationParameters(\n",
    "        decoding_method=DecodingMethod.SAMPLE,\n",
    "        max_new_tokens=100,\n",
    "        min_new_tokens=10,\n",
    "        temperature=0.5,\n",
    "        top_k=50,\n",
    "        top_p=1,\n",
    "        return_options=TextGenerationReturnOptions(input_text=False, input_tokens=True),\n",
    "    ),\n",
    "    moderations=ModerationParameters(\n",
    "        # Threshold is set to very low level to flag everything (testing purposes)\n",
    "        # or set to True to enable HAP with default settings\n",
    "        hap=ModerationHAP(input=ModerationHAPInput(enabled=True, threshold=0.01))\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(heading(\"Start conversation with langchain\"))\n",
    "prompt = \"Describe what is Python in one sentence.\"\n",
    "print(f\"Request: {prompt}\")\n",
    "result = llm.generate(\n",
    "    messages=[\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=\"\"\"You are a helpful, respectful and honest assistant.\n",
    "Always answer as helpfully as possible, while being safe.\n",
    "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
    "Please ensure that your responses are socially unbiased and positive in nature. If a question does not make\n",
    "any sense, or is not factually coherent, explain why instead of answering something incorrectly.\n",
    "If you don't know the answer to a question, please don't share false information.\n",
    "\"\"\",\n",
    "            ),\n",
    "            HumanMessage(content=prompt),\n",
    "        ]\n",
    "    ],\n",
    ")\n",
    "conversation_id = result.generations[0][0].generation_info[\"meta\"][\"conversation_id\"]\n",
    "print(f\"New conversation with ID '{conversation_id}' has been created!\")\n",
    "print(f\"Response: {result.generations[0][0].text}\")\n",
    "print(result.llm_output)\n",
    "print(result.generations[0][0].generation_info)\n",
    "\n",
    "print(heading(\"Continue conversation with langchain\"))\n",
    "prompt = \"Show me the example of code which can print fibonacci series.\"\n",
    "print(f\"Request: {prompt}\")\n",
    "result = llm.generate(\n",
    "    messages=[[HumanMessage(content=prompt)]], conversation_id=conversation_id, use_conversation_parameters=True\n",
    ")\n",
    "# prompt = \"Give me the fifth term for n =9.\"\n",
    "# print(f\"Request: {prompt}\")\n",
    "# result = llm.generate(\n",
    "#     messages=[[HumanMessage(content=prompt)]], conversation_id=conversation_id, use_conversation_parameters=True\n",
    "# )\n",
    "print(f\"Response: {result.generations[0][0].text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chat a model using langchain- IBM SDK - changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================== Start conversation with langchain =======================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from genai import Client, Credentials\n",
    "from genai.extensions.langchain.chat_llm import LangChainChatInterface\n",
    "from genai.schema import (\n",
    "    DecodingMethod,\n",
    "    ModerationHAP,\n",
    "    ModerationHAPInput,\n",
    "    ModerationParameters,\n",
    "    TextGenerationParameters,\n",
    "    TextGenerationReturnOptions,\n",
    ")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "def heading(text: str) -> str:\n",
    "    \"\"\"Helper function for centering text.\"\"\"\n",
    "    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n",
    "\n",
    "llm = LangChainChatInterface(\n",
    "    client=Client(credentials=Credentials.from_env()),\n",
    "    model_id=\"meta-llama/llama-2-70b-chat\",\n",
    "    parameters=TextGenerationParameters(\n",
    "        decoding_method=DecodingMethod.SAMPLE,\n",
    "        max_new_tokens=100,\n",
    "        min_new_tokens=10,\n",
    "        temperature=0.5,\n",
    "        top_k=50,\n",
    "        top_p=1,\n",
    "        return_options=TextGenerationReturnOptions(input_text=False, input_tokens=True),\n",
    "    ),\n",
    "    moderations=ModerationParameters(\n",
    "        hap=ModerationHAP(input=ModerationHAPInput(enabled=True, threshold=0.01))\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(heading(\"Start conversation with langchain\"))\n",
    "user_input = input(\"Enter your question: \")  # Prompt the user for input\n",
    "result = llm.generate(\n",
    "    messages=[\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=\"\"\"You are a helpful, respectful and honest assistant.\n",
    "Always answer as helpfully as possible, while being safe.\n",
    "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
    "Please ensure that your responses are socially unbiased and positive in nature. If a question does not make\n",
    "any sense, or is not factually coherent, explain why instead of answering something incorrectly.\n",
    "If you don't know the answer to a question, please don't share false information.\n",
    "\"\"\",\n",
    "            ),\n",
    "            HumanMessage(content=user_input),\n",
    "        ]\n",
    "    ],\n",
    ")\n",
    "conversation_id = result.generations[0][0].generation_info[\"meta\"][\"conversation_id\"]\n",
    "print(f\"New conversation with ID '{conversation_id}' has been created!\")\n",
    "print(f\"Response: {result.generations[0][0].text}\")\n",
    "print(result.llm_output)\n",
    "print(result.generations[0][0].generation_info)\n",
    "\n",
    "print(heading(\"Continue conversation with langchain\"))\n",
    "user_input = input(\"Enter your question: \")  # Prompt the user for input\n",
    "result = llm.generate(\n",
    "    messages=[[HumanMessage(content=user_input)]], conversation_id=conversation_id, use_conversation_parameters=True\n",
    ")\n",
    "print(f\"Response: {result.generations[0][0].text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================== Start conversation with langchain =======================\n",
      "\n",
      "Request: Describe what is Python in one sentence.\n",
      "An error occurred: Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 109\u001b[0m\n\u001b[1;32m    107\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDescribe what is Python in one sentence.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequest: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 109\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mlangchain_workflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m            \u001b[49m\u001b[43mSystemMessage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[38;5;124;43mYou are a helpful, respectful and honest assistant.\u001b[39;49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;43m                        Always answer as helpfully as possible, while being safe.\u001b[39;49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;43m                        Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\u001b[39;49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;43m                        Please ensure that your responses are socially unbiased and positive in nature. If a question does not make\u001b[39;49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;43m                        any sense, or is not factually coherent, explain why instead of answering something incorrectly.\u001b[39;49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;43m                        If you don\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mt know the answer to a question, please don\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mt share false information.\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m                \u001b[49m\u001b[43muser_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43massistant\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    120\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m            \u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mability\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpython-description\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfigurable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msession_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msession_id\u001b[49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m conversation_id \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mgeneration_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconversation_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew conversation with ID \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversation_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has been created!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Downloads/Langchain/langchain_agent/lib/python3.12/site-packages/langchain_core/runnables/base.py:3142\u001b[0m, in \u001b[0;36mRunnableParallel.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   3129\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m   3130\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   3131\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m   3132\u001b[0m                 step\u001b[38;5;241m.\u001b[39minvoke,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3140\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   3141\u001b[0m         ]\n\u001b[0;32m-> 3142\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[1;32m   3143\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3144\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/Downloads/Langchain/langchain_agent/lib/python3.12/site-packages/langchain_core/runnables/base.py:4525\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4520\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4521\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4522\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4523\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4524\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4526\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4527\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4528\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4529\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/Langchain/langchain_agent/lib/python3.12/site-packages/langchain_core/runnables/base.py:4525\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4520\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4521\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4522\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4523\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4524\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4526\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4527\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4528\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4529\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/Langchain/langchain_agent/lib/python3.12/site-packages/langchain_core/runnables/base.py:2499\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2497\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2498\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2499\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2500\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2501\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2502\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2503\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2504\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2505\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2506\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2507\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Downloads/Langchain/langchain_agent/lib/python3.12/site-packages/langchain_core/runnables/base.py:4525\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4520\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4521\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4522\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4523\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4524\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4526\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4527\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4528\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4529\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/Langchain/langchain_agent/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:159\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    154\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    155\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    157\u001b[0m         ChatGeneration,\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m--> 159\u001b[0m             [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m],\n\u001b[1;32m    160\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    161\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    162\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    163\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    164\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    165\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    166\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    167\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    168\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/Downloads/Langchain/langchain_agent/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:142\u001b[0m, in \u001b[0;36mBaseChatModel._convert_input\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatPromptValue(messages\u001b[38;5;241m=\u001b[39mconvert_to_messages(\u001b[38;5;28minput\u001b[39m))\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust be a PromptValue, str, or list of BaseMessages.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages."
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from genai import Client, Credentials\n",
    "from genai.extensions.langchain.chat_llm import LangChainChatInterface\n",
    "from genai.extensions.langchain import LangChainInterface\n",
    "from genai.schema import (\n",
    "    DecodingMethod,\n",
    "    ModerationHAP,\n",
    "    ModerationHAPInput,\n",
    "    ModerationParameters,\n",
    "    TextGenerationParameters,\n",
    "    TextGenerationReturnOptions,\n",
    ")\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel, RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "\n",
    "# Placeholder for retriever - Define or import the retriever module here\n",
    "retriever = None\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "def heading(text: str) -> str:\n",
    "    \"\"\"Helper function for centering text.\"\"\"\n",
    "    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n",
    "\n",
    "def langchain_interaction(msg: dict, config: dict):\n",
    "    try:\n",
    "        return agent_with_chat_history.invoke(msg, config)\n",
    "    except Exception as e:\n",
    "        # Handle the exception here\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        # You can log the error, retry, or take other actions as needed\n",
    "\n",
    "# Create the custom RunnableLambda for LangChain interactions\n",
    "langchain_error_handler = RunnableLambda(langchain_interaction)\n",
    "\n",
    "# Create the custom template prompt\n",
    "client = Client(credentials=Credentials.from_env())\n",
    "prompt_response = client.prompt.create(\n",
    "    name=\"Recipe Generator Prompt\",\n",
    "    model_id=\"google/flan-t5-xl\",\n",
    "    input=\"Make a short recipe for {{meal}} (use bullet points)\",\n",
    ")\n",
    "\n",
    "# Create the LangChain Chat Interface with memory functionality\n",
    "message_history = ChatMessageHistory()\n",
    "agent_with_chat_history = RunnableWithMessageHistory(\n",
    "    LangChainChatInterface(\n",
    "        client=client,\n",
    "        model_id=\"ibm/granite-13b-instruct-v2\",\n",
    "        prompt_id=prompt_response.result.id,\n",
    "        parameters=TextGenerationParameters(\n",
    "            decoding_method=DecodingMethod.SAMPLE,\n",
    "            max_new_tokens=100,\n",
    "            min_new_tokens=10,\n",
    "            temperature=0.5,\n",
    "            top_k=50,\n",
    "            top_p=1,\n",
    "            return_options=TextGenerationReturnOptions(input_text=False, input_tokens=True),\n",
    "        ),\n",
    "        moderations=ModerationParameters(\n",
    "            hap=ModerationHAP(input=ModerationHAPInput(enabled=True, threshold=0.01))\n",
    "        ),\n",
    "        data={\"meal\": \"Lasagne\"},\n",
    "    ),\n",
    "    lambda session_id: message_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "# def memory_runnable(x):\n",
    "#     # Implement the logic to handle the chat history and other data\n",
    "#     return {\"ability\": x[\"ability\"], \"chat_history\": x[\"chat_history\"], \"question\": x[\"question\"]}\n",
    "def memory_runnable(x):\n",
    "    ability = x.get(\"ability\")\n",
    "    question = x.get(\"question\")\n",
    "    chat_history = x.get(\"chat_history\", [])  # Default to an empty list if 'chat_history' key is not present\n",
    "    return {\"ability\": ability, \"chat_history\": chat_history, \"question\": question}\n",
    "\n",
    "# Add the setup_and_retrieval code block\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\n",
    "        \"response_from_vector_store\": itemgetter(\"question\") | (retriever if retriever else RunnablePassthrough()),\n",
    "        \"response_to_chat_history\": RunnableLambda(memory_runnable) | memory_runnable,\n",
    "        \"question\": RunnablePassthrough.assign(question=itemgetter(\"question\")),\n",
    "        \"ability\": RunnablePassthrough.assign(ability=itemgetter(\"ability\")),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Use the RunnableLambda in the LangChain workflow\n",
    "# langchain_workflow = langchain_error_handler.chain(setup_and_retrieval, agent_with_chat_history)\n",
    "langchain_workflow = RunnableParallel(\n",
    "    {\n",
    "        \"langchain_error_handler\": langchain_error_handler,\n",
    "        \"setup_and_retrieval\": setup_and_retrieval,\n",
    "        \"agent_with_chat_history\": agent_with_chat_history\n",
    "    }\n",
    ")\n",
    "\n",
    "# Start the conversation with memory functionality\n",
    "print(heading(\"Start conversation with langchain\"))\n",
    "prompt = \"Describe what is Python in one sentence.\"\n",
    "print(f\"Request: {prompt}\")\n",
    "result = langchain_workflow.invoke(\n",
    "    {\n",
    "        \"input\": [\n",
    "            SystemMessage(\n",
    "                content=\"\"\"You are a helpful, respectful and honest assistant.\n",
    "                        Always answer as helpfully as possible, while being safe.\n",
    "                        Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
    "                        Please ensure that your responses are socially unbiased and positive in nature. If a question does not make\n",
    "                        any sense, or is not factually coherent, explain why instead of answering something incorrectly.\n",
    "                        If you don't know the answer to a question, please don't share false information.\"\"\",\n",
    "                user_id=\"assistant\"\n",
    "            ),\n",
    "            HumanMessage(content=prompt, user_id=\"user\")\n",
    "        ],\n",
    "        \"ability\": \"python-description\",\n",
    "        \"question\": prompt,\n",
    "    },\n",
    "    config = {\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "\n",
    "conversation_id = result.generations[0][0].generation_info[\"meta\"][\"conversation_id\"]\n",
    "print(f\"New conversation with ID '{conversation_id}' has been created!\")\n",
    "print(f\"Response: {result.generations[0][0].text}\")\n",
    "print(result.llm_output)\n",
    "print(result.generations[0][0].generation_info)\n",
    "\n",
    "# Continue the conversation with memory functionality\n",
    "print(heading(\"Continue conversation with langchain\"))\n",
    "prompt = \"Show me some simple code example.\"\n",
    "print(f\"Request: {prompt}\")\n",
    "result = langchain_workflow.invoke(\n",
    "    {\n",
    "        \"input\": [HumanMessage(content=prompt, user_id=\"user\")],\n",
    "        \"ability\": \"code-example\",\n",
    "        \"question\": prompt,\n",
    "    },\n",
    "    {\"configurable\": {\"session_id\": conversation_id}},\n",
    "    use_conversation_parameters=True,\n",
    ")\n",
    "print(f\"Response: {result.generations[0][0].text}\")\n",
    "\n",
    "# Delete the prompt if no longer needed\n",
    "client.prompt.delete(prompt_response.result.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================== Start conversation with langchain =======================\n",
      "\n",
      "Request: Describe what is Python in one sentence.\n",
      "New conversation with ID 'c82f8ef3-9710-4a86-9772-611d6b4d89a4' has been created!\n",
      "Response:   Sure! Here's an example of Python code that prints the Fibonacci series up to a given number:\n",
      "```\n",
      "def fibonacci(n):\n",
      "    a, b = 0, 1\n",
      "    for i in range(n):\n",
      "        print(b, end=\" \")\n",
      "        a, b = b, a + b\n",
      "    print()\n",
      "\n",
      "# Example usage:\n",
      "fibonacci(5)\n",
      "```\n",
      "This code defines a\n",
      "{'model_name': 'meta-llama/llama-2-70b-chat', 'token_usage': {'prompt_tokens': 218, 'completion_tokens': 100, 'total_tokens': 318}}\n",
      "{'meta': {'conversation_id': 'c82f8ef3-9710-4a86-9772-611d6b4d89a4', 'created_at': datetime.datetime(2024, 5, 7, 2, 44, 11, 486000, tzinfo=TzInfo(UTC)), 'id': 'd4a2e560-93eb-4d0f-9a14-00fad9a231db', 'model_id': 'meta-llama/llama-2-70b-chat'}, 'token_usage': {'prompt_tokens': 218, 'completion_tokens': 100, 'total_tokens': 318}, 'generated_token_count': 100, 'input_token_count': 218, 'input_tokens': [{'text': '<s>'}, {'text': '▁['}, {'text': 'INST'}, {'text': ']'}, {'text': '▁<<'}, {'text': 'SY'}, {'text': 'S'}, {'text': '>>'}, {'text': '<0x0A>'}, {'text': 'You'}, {'text': '▁are'}, {'text': '▁a'}, {'text': '▁helpful'}, {'text': ','}, {'text': '▁respect'}, {'text': 'ful'}, {'text': '▁and'}, {'text': '▁honest'}, {'text': '▁assistant'}, {'text': '.'}, {'text': '<0x0A>'}, {'text': 'Al'}, {'text': 'ways'}, {'text': '▁answer'}, {'text': '▁as'}, {'text': '▁help'}, {'text': 'fully'}, {'text': '▁as'}, {'text': '▁possible'}, {'text': ','}, {'text': '▁while'}, {'text': '▁being'}, {'text': '▁safe'}, {'text': '.'}, {'text': '<0x0A>'}, {'text': 'Your'}, {'text': '▁answers'}, {'text': '▁should'}, {'text': '▁not'}, {'text': '▁include'}, {'text': '▁any'}, {'text': '▁harm'}, {'text': 'ful'}, {'text': ','}, {'text': '▁un'}, {'text': 'eth'}, {'text': 'ical'}, {'text': ','}, {'text': '▁rac'}, {'text': 'ist'}, {'text': ','}, {'text': '▁sex'}, {'text': 'ist'}, {'text': ','}, {'text': '▁to'}, {'text': 'xic'}, {'text': ','}, {'text': '▁dangerous'}, {'text': ','}, {'text': '▁or'}, {'text': '▁illegal'}, {'text': '▁content'}, {'text': '.'}, {'text': '<0x0A>'}, {'text': 'Please'}, {'text': '▁ensure'}, {'text': '▁that'}, {'text': '▁your'}, {'text': '▁responses'}, {'text': '▁are'}, {'text': '▁soci'}, {'text': 'ally'}, {'text': '▁un'}, {'text': 'bi'}, {'text': 'ased'}, {'text': '▁and'}, {'text': '▁positive'}, {'text': '▁in'}, {'text': '▁nature'}, {'text': '.'}, {'text': '▁If'}, {'text': '▁a'}, {'text': '▁question'}, {'text': '▁does'}, {'text': '▁not'}, {'text': '▁make'}, {'text': '<0x0A>'}, {'text': 'any'}, {'text': '▁sense'}, {'text': ','}, {'text': '▁or'}, {'text': '▁is'}, {'text': '▁not'}, {'text': '▁fact'}, {'text': 'ually'}, {'text': '▁coh'}, {'text': 'er'}, {'text': 'ent'}, {'text': ','}, {'text': '▁explain'}, {'text': '▁why'}, {'text': '▁instead'}, {'text': '▁of'}, {'text': '▁answering'}, {'text': '▁something'}, {'text': '▁incorrectly'}, {'text': '.'}, {'text': '<0x0A>'}, {'text': 'If'}, {'text': '▁you'}, {'text': '▁don'}, {'text': \"'\"}, {'text': 't'}, {'text': '▁know'}, {'text': '▁the'}, {'text': '▁answer'}, {'text': '▁to'}, {'text': '▁a'}, {'text': '▁question'}, {'text': ','}, {'text': '▁please'}, {'text': '▁don'}, {'text': \"'\"}, {'text': 't'}, {'text': '▁share'}, {'text': '▁false'}, {'text': '▁information'}, {'text': '.'}, {'text': '<0x0A>'}, {'text': '<0x0A>'}, {'text': '<'}, {'text': '</'}, {'text': 'SY'}, {'text': 'S'}, {'text': '>>'}, {'text': '<0x0A>'}, {'text': '<0x0A>'}, {'text': 'Des'}, {'text': 'cribe'}, {'text': '▁what'}, {'text': '▁is'}, {'text': '▁Python'}, {'text': '▁in'}, {'text': '▁one'}, {'text': '▁sentence'}, {'text': '.'}, {'text': '▁['}, {'text': '/'}, {'text': 'INST'}, {'text': ']'}, {'text': '▁▁'}, {'text': '▁Python'}, {'text': '▁is'}, {'text': '▁a'}, {'text': '▁high'}, {'text': '-'}, {'text': 'level'}, {'text': ','}, {'text': '▁interpreted'}, {'text': '▁programming'}, {'text': '▁language'}, {'text': '▁that'}, {'text': '▁is'}, {'text': '▁widely'}, {'text': '▁used'}, {'text': '▁for'}, {'text': '▁various'}, {'text': '▁purposes'}, {'text': '▁such'}, {'text': '▁as'}, {'text': '▁web'}, {'text': '▁development'}, {'text': ','}, {'text': '▁data'}, {'text': '▁analysis'}, {'text': ','}, {'text': '▁artificial'}, {'text': '▁intelligence'}, {'text': ','}, {'text': '▁and'}, {'text': '▁scientific'}, {'text': '▁computing'}, {'text': ','}, {'text': '▁known'}, {'text': '▁for'}, {'text': '▁its'}, {'text': '▁simplicity'}, {'text': ','}, {'text': '▁read'}, {'text': 'ability'}, {'text': ','}, {'text': '▁and'}, {'text': '▁ease'}, {'text': '▁of'}, {'text': '▁use'}, {'text': '.'}, {'text': '▁['}, {'text': 'INST'}, {'text': ']'}, {'text': '<0x0A>'}, {'text': 'Show'}, {'text': '▁me'}, {'text': '▁the'}, {'text': '▁example'}, {'text': '▁of'}, {'text': '▁code'}, {'text': '▁which'}, {'text': '▁can'}, {'text': '▁print'}, {'text': '▁fib'}, {'text': 'on'}, {'text': 'acci'}, {'text': '▁series'}, {'text': '.'}, {'text': '▁['}, {'text': '/'}, {'text': 'INST'}, {'text': ']'}], 'moderations': {'hap': [{'flagged': True, 'position': {'end': 60, 'start': 0}, 'score': 0.1330908089876175, 'success': True}]}, 'seed': 1863829818.0, 'stop_reason': 'max_tokens', 'moderation': {'hap': [{'flagged': True, 'position': {'end': 60, 'start': 0}, 'score': 0.1330908089876175, 'success': True}]}}\n",
      "\n",
      "===================== Continue conversation with langchain =====================\n",
      "\n",
      "Request: Show me some simple code example.\n",
      "An error occurred: Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 139\u001b[0m\n\u001b[1;32m    137\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShow me some simple code example.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequest: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 139\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mlangchain_workflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# \"input\": HumanMessage(content=prompt, user_id=\"user\"),\u001b[39;49;00m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mShow me some simple code example\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mability\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcode-example\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mShow me some simple code example\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfigurable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msession_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconversation_id\u001b[49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# use_conversation_parameters=True\u001b[39;49;00m\n\u001b[1;32m    148\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# Delete the prompt if no longer needed\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/Langchain/langchain_agent/lib/python3.12/site-packages/langchain_core/runnables/base.py:3142\u001b[0m, in \u001b[0;36mRunnableParallel.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   3129\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m   3130\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   3131\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m   3132\u001b[0m                 step\u001b[38;5;241m.\u001b[39minvoke,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3140\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   3141\u001b[0m         ]\n\u001b[0;32m-> 3142\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[1;32m   3143\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3144\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/Downloads/Langchain/langchain_agent/lib/python3.12/site-packages/langchain_core/runnables/base.py:4525\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4520\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4521\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4522\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4523\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4524\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4526\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4527\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4528\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4529\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/Langchain/langchain_agent/lib/python3.12/site-packages/langchain_core/runnables/base.py:4525\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4520\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4521\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4522\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4523\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4524\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4526\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4527\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4528\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4529\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/Langchain/langchain_agent/lib/python3.12/site-packages/langchain_core/runnables/base.py:2499\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2497\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2498\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2499\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2500\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2501\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2502\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2503\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2504\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2505\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2506\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2507\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Downloads/Langchain/langchain_agent/lib/python3.12/site-packages/langchain_core/runnables/base.py:4525\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4520\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4521\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4522\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4523\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4524\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4526\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4527\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4528\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4529\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/Langchain/langchain_agent/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:159\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    154\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    155\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    157\u001b[0m         ChatGeneration,\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m--> 159\u001b[0m             [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m],\n\u001b[1;32m    160\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    161\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    162\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    163\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    164\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    165\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    166\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    167\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    168\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/Downloads/Langchain/langchain_agent/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:142\u001b[0m, in \u001b[0;36mBaseChatModel._convert_input\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatPromptValue(messages\u001b[38;5;241m=\u001b[39mconvert_to_messages(\u001b[38;5;28minput\u001b[39m))\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust be a PromptValue, str, or list of BaseMessages.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages."
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from genai import Client, Credentials\n",
    "from genai.extensions.langchain.chat_llm import LangChainChatInterface\n",
    "from genai.extensions.langchain import LangChainInterface\n",
    "from genai.schema import (\n",
    "    DecodingMethod,\n",
    "    ModerationHAP,\n",
    "    ModerationHAPInput,\n",
    "    ModerationParameters,\n",
    "    TextGenerationParameters,\n",
    "    TextGenerationReturnOptions,\n",
    ")\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel, RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "\n",
    "# Placeholder for retriever - Define or import the retriever module here\n",
    "retriever = None\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "def heading(text: str) -> str:\n",
    "    \"\"\"Helper function for centering text.\"\"\"\n",
    "    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n",
    "\n",
    "def langchain_interaction(msg: dict, config: dict):\n",
    "    try:\n",
    "        return agent_with_chat_history.invoke(msg, config)\n",
    "    except Exception as e:\n",
    "        # Handle the exception here\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        # You can log the error, retry, or take other actions as needed\n",
    "\n",
    "# Create the custom RunnableLambda for LangChain interactions\n",
    "langchain_error_handler = RunnableLambda(langchain_interaction)\n",
    "\n",
    "# Create the custom template prompt\n",
    "client = Client(credentials=Credentials.from_env())\n",
    "prompt_response = client.prompt.create(\n",
    "    name=\"Recipe Generator Prompt\",\n",
    "    model_id=\"google/flan-t5-xl\",\n",
    "    input=\"Make a short recipe for {{meal}} (use bullet points)\",\n",
    ")\n",
    "\n",
    "# Create the LangChain Chat Interface with memory functionality\n",
    "message_history = ChatMessageHistory()\n",
    "agent_with_chat_history = RunnableWithMessageHistory(\n",
    "    LangChainChatInterface(\n",
    "        client=client,\n",
    "        model_id=\"ibm/granite-13b-instruct-v2\",\n",
    "        prompt_id=prompt_response.result.id,\n",
    "        parameters=TextGenerationParameters(\n",
    "            decoding_method=DecodingMethod.SAMPLE,\n",
    "            max_new_tokens=100,\n",
    "            min_new_tokens=10,\n",
    "            temperature=0.5,\n",
    "            top_k=50,\n",
    "            top_p=1,\n",
    "            return_options=TextGenerationReturnOptions(input_text=False, input_tokens=True),\n",
    "        ),\n",
    "        moderations=ModerationParameters(\n",
    "            hap=ModerationHAP(input=ModerationHAPInput(enabled=True, threshold=0.01))\n",
    "        ),\n",
    "        data={\"meal\": \"Lasagne\"},\n",
    "    ),\n",
    "    lambda session_id: message_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "# def memory_runnable(x):\n",
    "#     # Implement the logic to handle the chat history and other data\n",
    "#     return {\"ability\": x[\"ability\"], \"chat_history\": x[\"chat_history\"], \"question\": x[\"question\"]}\n",
    "def memory_runnable(x):\n",
    "    ability = x.get(\"ability\")\n",
    "    question = x.get(\"question\")\n",
    "    chat_history = x.get(\"chat_history\", [])  # Default to an empty list if 'chat_history' key is not present\n",
    "    return {\"ability\": ability, \"chat_history\": chat_history, \"question\": question}\n",
    "\n",
    "# Add the setup_and_retrieval code block\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\n",
    "        \"response_from_vector_store\": itemgetter(\"question\") | (retriever if retriever else RunnablePassthrough()),\n",
    "        \"response_to_chat_history\": RunnableLambda(memory_runnable) | memory_runnable,\n",
    "        \"question\": RunnablePassthrough.assign(question=itemgetter(\"question\")),\n",
    "        \"ability\": RunnablePassthrough.assign(ability=itemgetter(\"ability\")),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Use the RunnableLambda in the LangChain workflow\n",
    "# langchain_workflow = langchain_error_handler.chain(setup_and_retrieval, agent_with_chat_history)\n",
    "langchain_workflow = RunnableParallel(\n",
    "    {\n",
    "        \"langchain_error_handler\": langchain_error_handler,\n",
    "        \"setup_and_retrieval\": setup_and_retrieval,\n",
    "        \"agent_with_chat_history\": agent_with_chat_history\n",
    "    }\n",
    ")\n",
    "\n",
    "# Start the conversation with memory functionality\n",
    "print(heading(\"Start conversation with langchain\"))\n",
    "prompt = \"Describe what is Python in one sentence.\"\n",
    "print(f\"Request: {prompt}\")\n",
    "# input_messages = [\n",
    "#     SystemMessage(content=\"\"\"\"You are a helpful, respectful and honest assistant.\n",
    "#                         Always answer as helpfully as possible, while being safe.\n",
    "#                         Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
    "#                         Please ensure that your responses are socially unbiased and positive in nature. If a question does not make\n",
    "#                         any sense, or is not factually coherent, explain why instead of answering something incorrectly.\n",
    "#                         If you don't know the answer to a question, please don't share false information.\n",
    "#                             \"\"\" , user_id=\"assistant\"),\n",
    "#     HumanMessage(content=prompt, user_id=\"user\")\n",
    "# ]\n",
    "\n",
    "# result = langchain_workflow.invoke(\n",
    "#     {\n",
    "#         \"input\": input_messages,\n",
    "#         \"ability\": \"python-description\",\n",
    "#         \"question\": prompt,\n",
    "#     },\n",
    "#     config = {\"configurable\": {\"session_id\": session_id}}\n",
    "# )\n",
    "\n",
    "conversation_id = result.generations[0][0].generation_info[\"meta\"][\"conversation_id\"]\n",
    "print(f\"New conversation with ID '{conversation_id}' has been created!\")\n",
    "print(f\"Response: {result.generations[0][0].text}\")\n",
    "print(result.llm_output)\n",
    "print(result.generations[0][0].generation_info)\n",
    "\n",
    "# Continue the conversation with memory functionality\n",
    "print(heading(\"Continue conversation with langchain\"))\n",
    "prompt = \"Show me some simple code example.\"\n",
    "print(f\"Request: {prompt}\")\n",
    "result = langchain_workflow.invoke(\n",
    "    {\n",
    "        # \"input\": HumanMessage(content=prompt, user_id=\"user\"),\n",
    "        \"input\":\"Show me some simple code example\",\n",
    "        \"ability\": \"code-example\",\n",
    "        \"question\": \"Show me some simple code example\",\n",
    "    },\n",
    "    {\"configurable\": {\"session_id\": conversation_id}},\n",
    "    # use_conversation_parameters=True\n",
    ")\n",
    "print(f\"Response: {result.generations[0][0].text}\")\n",
    "\n",
    "# Delete the prompt if no longer needed\n",
    "client.prompt.delete(prompt_response.result.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changed code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================== Start conversation with langchain =======================\n",
      "\n",
      "Request: Describe what is Python in one sentence.\n",
      "An error occurred: The input to RunnablePassthrough.assign() must be a dict.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "The input to RunnablePassthrough.assign() must be a dict.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 147\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    142\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput,\n\u001b[1;32m    143\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mability\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mability,\n\u001b[1;32m    144\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquestion\n\u001b[1;32m    145\u001b[0m         }\n\u001b[1;32m    146\u001b[0m prompt_value \u001b[38;5;241m=\u001b[39m PromptValue(input_messages\u001b[38;5;241m=\u001b[39m[input_message], ability\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython-description\u001b[39m\u001b[38;5;124m\"\u001b[39m, question\u001b[38;5;241m=\u001b[39mprompt)\n\u001b[0;32m--> 147\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mlangchain_workflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfigurable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msession_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<foo>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# result = langchain_workflow.invoke(\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m#     {\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m#         \"input\": [HumanMessage(content=prompt, user_id=\"user\")],\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m#     config={\"configurable\": {\"session_id\": \"<foo>\"}}\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m    157\u001b[0m conversation_id \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mgeneration_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconversation_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Downloads/Langchain/langchain_agent/lib/python3.12/site-packages/langchain_core/runnables/base.py:3142\u001b[0m, in \u001b[0;36mRunnableParallel.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   3129\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m   3130\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   3131\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m   3132\u001b[0m                 step\u001b[38;5;241m.\u001b[39minvoke,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3140\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   3141\u001b[0m         ]\n\u001b[0;32m-> 3142\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[1;32m   3143\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3144\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/Downloads/Langchain/langchain_agent/lib/python3.12/site-packages/langchain_core/runnables/base.py:4525\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4520\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4521\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4522\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4523\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4524\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4526\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4527\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4528\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4529\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/Langchain/langchain_agent/lib/python3.12/site-packages/langchain_core/runnables/base.py:4525\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4520\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4521\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4522\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4523\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4524\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4526\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4527\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4528\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4529\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/Langchain/langchain_agent/lib/python3.12/site-packages/langchain_core/runnables/base.py:2499\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2497\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2498\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2499\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2500\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2501\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2502\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2503\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2504\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2505\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2506\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2507\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Downloads/Langchain/langchain_agent/lib/python3.12/site-packages/langchain_core/runnables/base.py:4525\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4520\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4521\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4522\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4523\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4524\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4526\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4527\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4528\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4529\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/Langchain/langchain_agent/lib/python3.12/site-packages/langchain_core/runnables/passthrough.py:469\u001b[0m, in \u001b[0;36mRunnableAssign.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28minput\u001b[39m: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    466\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    468\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m--> 469\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/Langchain/langchain_agent/lib/python3.12/site-packages/langchain_core/runnables/base.py:1626\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1623\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(var_child_runnable_config\u001b[38;5;241m.\u001b[39mset, child_config)\n\u001b[1;32m   1624\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1625\u001b[0m         Output,\n\u001b[0;32m-> 1626\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1628\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1629\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1630\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1634\u001b[0m     )\n\u001b[1;32m   1635\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1636\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/Downloads/Langchain/langchain_agent/lib/python3.12/site-packages/langchain_core/runnables/config.py:347\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    346\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/Langchain/langchain_agent/lib/python3.12/site-packages/langchain_core/runnables/passthrough.py:450\u001b[0m, in \u001b[0;36mRunnableAssign._invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_invoke\u001b[39m(\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28minput\u001b[39m: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    449\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m--> 450\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mdict\u001b[39m\n\u001b[1;32m    452\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe input to RunnablePassthrough.assign() must be a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    455\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    456\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmapper\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    460\u001b[0m         ),\n\u001b[1;32m    461\u001b[0m     }\n",
      "\u001b[0;31mAssertionError\u001b[0m: The input to RunnablePassthrough.assign() must be a dict."
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from genai import Client, Credentials\n",
    "from genai.extensions.langchain.chat_llm import LangChainChatInterface\n",
    "from genai.extensions.langchain import LangChainInterface\n",
    "from genai.schema import (\n",
    "    DecodingMethod,\n",
    "    ModerationHAP,\n",
    "    ModerationHAPInput,\n",
    "    ModerationParameters,\n",
    "    TextGenerationParameters,\n",
    "    TextGenerationReturnOptions,\n",
    ")\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel, RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# Placeholder for retriever - Define or import the retriever module here\n",
    "retriever = None\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "def heading(text: str) -> str:\n",
    "    \"\"\"Helper function for centering text.\"\"\"\n",
    "    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n",
    "\n",
    "def langchain_interaction(msg: dict, config: dict):\n",
    "    try:\n",
    "        return agent_with_chat_history.invoke(msg, config)\n",
    "    except Exception as e:\n",
    "        # Handle the exception here\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        # You can log the error, retry, or take other actions as needed\n",
    "\n",
    "# Create the custom RunnableLambda for LangChain interactions\n",
    "langchain_error_handler = RunnableLambda(langchain_interaction)\n",
    "\n",
    "# Create the custom template prompt\n",
    "client = Client(credentials=Credentials.from_env())\n",
    "prompt_response = client.prompt.create(\n",
    "    name=\"Recipe Generator Prompt\",\n",
    "    model_id=\"google/flan-t5-xl\",\n",
    "    input=\"Make a short recipe for {{meal}} (use bullet points)\",\n",
    ")\n",
    "\n",
    "# Create the LangChain Chat Interface with memory functionality\n",
    "message_history = ChatMessageHistory()\n",
    "agent_with_chat_history = RunnableWithMessageHistory(\n",
    "    LangChainChatInterface(\n",
    "        client=client,\n",
    "        model_id=\"ibm/granite-13b-instruct-v2\",\n",
    "        prompt_id=prompt_response.result.id,\n",
    "        parameters=TextGenerationParameters(\n",
    "            decoding_method=DecodingMethod.SAMPLE,\n",
    "            max_new_tokens=100,\n",
    "            min_new_tokens=10,\n",
    "            temperature=0.5,\n",
    "            top_k=50,\n",
    "            top_p=1,\n",
    "            return_options=TextGenerationReturnOptions(input_text=False, input_tokens=True),\n",
    "        ),\n",
    "        moderations=ModerationParameters(\n",
    "            hap=ModerationHAP(input=ModerationHAPInput(enabled=True, threshold=0.01))\n",
    "        ),\n",
    "        data={\"meal\": \"Lasagne\"},\n",
    "    ),\n",
    "    lambda session_id: message_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "# Define the setup_and_retrieval workflow\n",
    "# setup_and_retrieval = RunnableParallel(\n",
    "#     {\n",
    "#         \"response_from_vector_store\": itemgetter(\"question\") | (retriever if retriever else RunnablePassthrough()),\n",
    "#         \"response_to_chat_history\": RunnableLambda(lambda x: x) | RunnablePassthrough(),\n",
    "#         \"question\": RunnablePassthrough.assign(question=itemgetter(\"question\")),\n",
    "#         \"ability\": RunnablePassthrough.assign(ability=itemgetter(\"ability\")),\n",
    "#     }\n",
    "# )\n",
    "# Define the setup_and_retrieval workflow\n",
    "# setup_and_retrieval = RunnableParallel(\n",
    "#     {\n",
    "#         \"response_from_vector_store\": itemgetter(\"question\") | (retriever if retriever else RunnablePassthrough()),\n",
    "#         \"response_to_chat_history\": RunnableLambda(lambda x: x) | RunnablePassthrough(),\n",
    "#         \"question\": RunnablePassthrough.assign(question=lambda prompt_value: prompt_value.to_dict()[\"question\"]),\n",
    "#         \"ability\": RunnablePassthrough.assign(ability=lambda prompt_value: prompt_value.to_dict()[\"ability\"]),\n",
    "#     }\n",
    "# )\n",
    "# Define the setup_and_retrieval workflow\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\n",
    "        \"response_from_vector_store\": itemgetter(\"question\") | (retriever if retriever else RunnablePassthrough()),\n",
    "        \"response_to_chat_history\": RunnableLambda(lambda x: x) | RunnablePassthrough(),\n",
    "        \"question\": lambda prompt_value: prompt_value.to_dict()[\"question\"],\n",
    "        \"ability\": lambda prompt_value: prompt_value.to_dict()[\"ability\"],\n",
    "    }\n",
    ")\n",
    "# Define the LangChain workflow\n",
    "langchain_workflow = RunnableParallel(\n",
    "    {\n",
    "        \"langchain_error_handler\": langchain_error_handler,\n",
    "        \"setup_and_retrieval\": setup_and_retrieval,\n",
    "        \"agent_with_chat_history\": agent_with_chat_history\n",
    "    }\n",
    ")\n",
    "\n",
    "# Start the conversation with memory functionality\n",
    "print(heading(\"Start conversation with langchain\"))\n",
    "prompt = \"Describe what is Python in one sentence.\"\n",
    "print(f\"Request: {prompt}\")\n",
    "input_message = HumanMessage(content=prompt, user_id=\"user\")\n",
    "# class PromptValue:\n",
    "#     def __init__(self, input_messages, ability, question):\n",
    "#         self.input_messages = input_messages\n",
    "#         self.ability = ability\n",
    "#         self.question = question\n",
    "class PromptValue:\n",
    "    def __init__(self, input_messages, ability, question):\n",
    "        self.input = input_messages\n",
    "        self.ability = ability\n",
    "        self.question = question\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if key == \"input\":\n",
    "            return self.input\n",
    "        elif key == \"ability\":\n",
    "            return self.ability\n",
    "        elif key == \"question\":\n",
    "            return self.question\n",
    "        else:\n",
    "            raise KeyError(f\"Key '{key}' not found in PromptValue object.\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter([\"input\", \"ability\", \"question\"])\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"input\": self.input,\n",
    "            \"ability\": self.ability,\n",
    "            \"question\": self.question\n",
    "        }\n",
    "prompt_value = PromptValue(input_messages=[input_message], ability=\"python-description\", question=prompt)\n",
    "result = langchain_workflow.invoke(prompt_value, config={\"configurable\": {\"session_id\": \"<foo>\"}})\n",
    "# result = langchain_workflow.invoke(\n",
    "#     {\n",
    "#         \"input\": [HumanMessage(content=prompt, user_id=\"user\")],\n",
    "#         \"ability\": \"python-description\",\n",
    "#         \"question\": prompt,\n",
    "#     },\n",
    "#     config={\"configurable\": {\"session_id\": \"<foo>\"}}\n",
    "# )\n",
    "\n",
    "conversation_id = result.generations[0][0].generation_info[\"meta\"][\"conversation_id\"]\n",
    "print(f\"New conversation with ID '{conversation_id}' has been created!\")\n",
    "print(f\"Response: {result.generations[0][0].text}\")\n",
    "print(result.llm_output)\n",
    "print(result.generations[0][0].generation_info)\n",
    "\n",
    "# Continue the conversation with memory functionality\n",
    "print(heading(\"Continue conversation with langchain\"))\n",
    "prompt = \"Show me some simple code example.\"\n",
    "print(f\"Request: {prompt}\")\n",
    "\n",
    "result = langchain_workflow.invoke(\n",
    "    {\n",
    "        \"input\": [HumanMessage(content=prompt, user_id=\"user\")],\n",
    "        \"ability\": \"code-example\",\n",
    "        \"question\": prompt,\n",
    "    },\n",
    "    config={\"configurable\": {\"session_id\": conversation_id}}\n",
    ")\n",
    "\n",
    "print(f\"Response: {result.generations[0][0].text}\")\n",
    "\n",
    "# Delete the prompt if no longer needed\n",
    "client.prompt.delete(prompt_response.result.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Describe what is Python in one sentence.'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value.question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SystemMessagePromptTemplate' object has no attribute 'to_string'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m PromptTemplate(input_variables\u001b[38;5;241m=\u001b[39minput_variables, input_types\u001b[38;5;241m=\u001b[39minput_types, metadata\u001b[38;5;241m=\u001b[39mmetadata, messages\u001b[38;5;241m=\u001b[39mmessages)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Assuming you have the 'prompt' object from hub.pull(\"hwchase17/openai-functions-agent\")\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[43mChatPromptTemplateConverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Define your custom tools\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmy_tool\u001b[39m(query: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n",
      "Cell \u001b[0;32mIn[64], line 18\u001b[0m, in \u001b[0;36mChatPromptTemplateConverter.convert\u001b[0;34m(chat_prompt_template)\u001b[0m\n\u001b[1;32m     16\u001b[0m input_types \u001b[38;5;241m=\u001b[39m chat_prompt_template\u001b[38;5;241m.\u001b[39minput_types\n\u001b[1;32m     17\u001b[0m metadata \u001b[38;5;241m=\u001b[39m chat_prompt_template\u001b[38;5;241m.\u001b[39mmetadata\n\u001b[0;32m---> 18\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_string\u001b[49m() \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m chat_prompt_template\u001b[38;5;241m.\u001b[39mmessages]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m PromptTemplate(input_variables\u001b[38;5;241m=\u001b[39minput_variables, input_types\u001b[38;5;241m=\u001b[39minput_types, metadata\u001b[38;5;241m=\u001b[39mmetadata, messages\u001b[38;5;241m=\u001b[39mmessages)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SystemMessagePromptTemplate' object has no attribute 'to_string'"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_tool_calling_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.tools import Tool\n",
    "from langchain import hub\n",
    "import typing\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "prompt.messages\n",
    "\n",
    "class ChatPromptTemplateConverter:\n",
    "    @staticmethod\n",
    "    def convert(chat_prompt_template):\n",
    "        input_variables = chat_prompt_template.input_variables\n",
    "        input_types = chat_prompt_template.input_types\n",
    "        metadata = chat_prompt_template.metadata\n",
    "        messages = [message.to_string() for message in chat_prompt_template.messages]\n",
    "\n",
    "        return PromptTemplate(input_variables=input_variables, input_types=input_types, metadata=metadata, messages=messages)\n",
    "\n",
    "# Assuming you have the 'prompt' object from hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "prompt = ChatPromptTemplateConverter.convert(prompt)\n",
    "\n",
    "# Define your custom tools\n",
    "def my_tool(query: str) -> str:\n",
    "    \"\"\"A custom tool that can be used by the agent.\"\"\"\n",
    "    return f\"This is the result of using the '{query}' tool.\"\n",
    "\n",
    "tool = Tool(\n",
    "    name=\"my_tool\",\n",
    "    func=my_tool,\n",
    "    description=\"A custom tool that can be used by the agent.\"\n",
    ")\n",
    "\n",
    "# Create the agent\n",
    "llm = \"ibm/granite-13b-instruct-v2\"\n",
    "tools = [tool]\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "# Create the agent executor\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# Run the agent\n",
    "result = agent_executor.run(\"Use the my_tool tool to do something.\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using chatprompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for SystemMessagePromptTemplate\nprompt\n  field required (type=value_error.missing)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 75\u001b[0m\n\u001b[1;32m     49\u001b[0m agent_with_chat_history \u001b[38;5;241m=\u001b[39m RunnableWithMessageHistory(\n\u001b[1;32m     50\u001b[0m     LangChainChatInterface(\n\u001b[1;32m     51\u001b[0m         client\u001b[38;5;241m=\u001b[39mclient,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m     history_messages_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     71\u001b[0m )\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Create the ChatPromptTemplate\u001b[39;00m\n\u001b[1;32m     74\u001b[0m chat_prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages([\n\u001b[0;32m---> 75\u001b[0m     \u001b[43mSystemMessagePromptTemplate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSystemMessage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[38;5;124;43mThank you for engaging with me! I strive to provide accurate and helpful responses while maintaining a safe and respectful environment.\u001b[39;49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;43m            Your questions are valuable, and I\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mm here to assist you to the best of my abilities. Let\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms explore and learn together!\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43massistant\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     82\u001b[0m     HumanMessagePromptTemplate(\n\u001b[1;32m     83\u001b[0m         message\u001b[38;5;241m=\u001b[39mHumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{query}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, user_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     84\u001b[0m     )\n\u001b[1;32m     85\u001b[0m ])\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Define langchain_workflow\u001b[39;00m\n\u001b[1;32m     88\u001b[0m langchain_workflow \u001b[38;5;241m=\u001b[39m RunnableParallel(\n\u001b[1;32m     89\u001b[0m     {\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlangchain_error_handler\u001b[39m\u001b[38;5;124m\"\u001b[39m: langchain_error_handler,\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent_with_chat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m: agent_with_chat_history\n\u001b[1;32m     92\u001b[0m     }\n\u001b[1;32m     93\u001b[0m )\n",
      "File \u001b[0;32m~/Downloads/Langchain/langchain_agent/lib/python3.12/site-packages/langchain_core/load/serializable.py:120\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/Downloads/Langchain/langchain_agent/lib/python3.12/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for SystemMessagePromptTemplate\nprompt\n  field required (type=value_error.missing)"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from genai import Client, Credentials\n",
    "from genai.extensions.langchain.chat_llm import LangChainChatInterface\n",
    "from genai.extensions.langchain import LangChainInterface\n",
    "from genai.schema import (\n",
    "    DecodingMethod,\n",
    "    ModerationHAP,\n",
    "    ModerationHAPInput,\n",
    "    ModerationParameters,\n",
    "    TextGenerationParameters,\n",
    "    TextGenerationReturnOptions,\n",
    ")\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# Placeholder for retriever - Define or import the retriever module here\n",
    "retriever = None\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "def heading(text: str) -> str:\n",
    "    \"\"\"Helper function for centering text.\"\"\"\n",
    "    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n",
    "\n",
    "def langchain_interaction(msg: dict, config: dict):\n",
    "    try:\n",
    "        return agent_with_chat_history.invoke(msg, config)\n",
    "    except Exception as e:\n",
    "        # Handle the exception here\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Create the custom RunnableLambda for LangChain interactions\n",
    "langchain_error_handler = RunnableLambda(langchain_interaction)\n",
    "\n",
    "# Create the custom template prompt\n",
    "client = Client(credentials=Credentials.from_env())\n",
    "prompt_response = client.prompt.create(\n",
    "    name=\"Recipe Generator Prompt\",\n",
    "    model_id=\"google/flan-t5-xl\",\n",
    "    input=\"Make a short recipe for {{meal}} (use bullet points)\",\n",
    ")\n",
    "\n",
    "# Create the LangChain Chat Interface with memory functionality\n",
    "message_history = ChatMessageHistory()\n",
    "agent_with_chat_history = RunnableWithMessageHistory(\n",
    "    LangChainChatInterface(\n",
    "        client=client,\n",
    "        model_id=\"ibm/granite-13b-instruct-v2\",\n",
    "        prompt_id=prompt_response.result.id,\n",
    "        parameters=TextGenerationParameters(\n",
    "            decoding_method=DecodingMethod.SAMPLE,\n",
    "            max_new_tokens=100,\n",
    "            min_new_tokens=10,\n",
    "            temperature=0.5,\n",
    "            top_k=50,\n",
    "            top_p=1,\n",
    "            return_options=TextGenerationReturnOptions(input_text=False, input_tokens=True),\n",
    "        ),\n",
    "        moderations=ModerationParameters(\n",
    "            hap=ModerationHAP(input=ModerationHAPInput(enabled=True, threshold=0.01))\n",
    "        ),\n",
    "        data={\"meal\": \"Lasagne\"},\n",
    "    ),\n",
    "    lambda session_id: message_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "# Create the ChatPromptTemplate\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate(\n",
    "        message=SystemMessage(\n",
    "            content=\"\"\"Thank you for engaging with me! I strive to provide accurate and helpful responses while maintaining a safe and respectful environment.\n",
    "            Your questions are valuable, and I'm here to assist you to the best of my abilities. Let's explore and learn together!\"\"\",\n",
    "            user_id=\"assistant\"\n",
    "        )\n",
    "    ),\n",
    "    HumanMessagePromptTemplate(\n",
    "        message=HumanMessage(content=\"{query}\", user_id=\"user\")\n",
    "    )\n",
    "])\n",
    "\n",
    "# Define langchain_workflow\n",
    "langchain_workflow = RunnableParallel(\n",
    "    {\n",
    "        \"langchain_error_handler\": langchain_error_handler,\n",
    "        \"agent_with_chat_history\": agent_with_chat_history\n",
    "    }\n",
    ")\n",
    "\n",
    "# Start the conversation with memory functionality using ChatPromptTemplate\n",
    "print(heading(\"Start conversation with langchain\"))\n",
    "prompt = \"Describe what is Python in one sentence.\"\n",
    "print(f\"Request: {prompt}\")\n",
    "result = langchain_workflow.invoke(\n",
    "    chat_prompt.format_prompt(query=prompt).dict(),\n",
    "    {\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "\n",
    "# Start the conversation with memory functionality using RunnablePassthrough\n",
    "# print(heading(\"Start conversation with langchain\"))\n",
    "# prompt = \"Describe what is Python in one sentence.\"\n",
    "# print(f\"Request: {prompt}\")\n",
    "# result = langchain_workflow.invoke(\n",
    "#     RunnablePassthrough.assign(\n",
    "#         input=[\n",
    "#             SystemMessage(\n",
    "#                 content=\"\"\"Thank you for engaging with me! I strive to provide accurate and helpful responses while maintaining a safe and respectful environment.\n",
    "#                 Your questions are valuable, and I'm here to assist you to the best of my abilities. Let's explore and learn together!\"\"\",\n",
    "#                 user_id=\"assistant\"\n",
    "#             ),\n",
    "#             HumanMessage(content=prompt, user_id=\"user\")\n",
    "#         ],\n",
    "#         ability=\"python-description\",\n",
    "#         question=prompt\n",
    "#     ),\n",
    "#     {\"configurable\": {\"session_id\": session_id}}\n",
    "# )\n",
    "\n",
    "# Continue the conversation with memory functionality\n",
    "print(heading(\"Continue conversation with langchain\"))\n",
    "prompt = \"Show me some simple code example.\"\n",
    "print(f\"Request: {prompt}\")\n",
    "result = langchain_workflow.invoke(\n",
    "    {\n",
    "        \"input\": [HumanMessage(content=prompt, user_id=\"user\")],\n",
    "        \"ability\": \"code-example\",\n",
    "        \"question\": prompt,\n",
    "    },\n",
    "    {\"configurable\": {\"session_id\": conversation_id}},\n",
    "    use_conversation_parameters=True,\n",
    ")\n",
    "\n",
    "# Delete the prompt if no longer needed\n",
    "client.prompt.delete(prompt_response.result.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
